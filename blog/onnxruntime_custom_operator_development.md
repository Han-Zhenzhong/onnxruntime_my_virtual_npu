# ONNXRuntime è‡ªå®šä¹‰ç®—å­å¼€å‘å®æˆ˜ï¼šåŸºäº Virtual NPU çš„ FastGelu å®ç°

## å‰è¨€

åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦ä¸ºç‰¹å®šç¡¬ä»¶å®ç°è‡ªå®šä¹‰ç®—å­ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨ ONNXRuntime ä¸­å¼€å‘è‡ªå®šä¹‰ç®—å­ï¼Œä»¥ FastGelu ç®—å­ä¸ºä¾‹ï¼Œå±•ç¤ºä»ç®—å­æ³¨å†Œã€å†…æ ¸å®ç°åˆ°å•å…ƒæµ‹è¯•ã€å¤§æ¨¡å‹éªŒè¯çš„å®Œæ•´æµç¨‹ã€‚

## é¡¹ç›®èƒŒæ™¯

ONNXRuntime æ˜¯å¾®è½¯å¼€æºçš„é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶åç«¯ã€‚æœ¬æ–‡åŸºäº ONNXRuntime 1.20.0ï¼Œå®ç°äº†ä¸€ä¸ªè™šæ‹Ÿ NPU æ‰§è¡Œæä¾›å™¨ï¼ˆmy_cpu providerï¼‰ï¼Œç”¨äºæ¼”ç¤ºè‡ªå®šä¹‰ç®—å­çš„å¼€å‘æµç¨‹ã€‚

**æŠ€æœ¯æ ˆï¼š**
- ONNXRuntime 1.20.0
- C++17
- CMake æ„å»ºç³»ç»Ÿ
- Python 3.10
- è‡ªå®šä¹‰åŸŸåï¼š`com.my_virtual_npu`

## ä¸€ã€æ¶æ„è®¾è®¡

### 1.1 è‡ªå®šä¹‰åŸŸä¸ç®—å­æ³¨å†Œ

ä¸ºäº†é¿å…ä¸ ONNXRuntime å†…ç½®ç®—å­å†²çªï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰åŸŸåï¼š

```cpp
// onnxruntime/core/providers/my_cpu/my_virtual_npu_defs.h
namespace onnxruntime {
namespace contrib {

constexpr const char* kMyCustomDomain = "com.my_virtual_npu";

// æ³¨å†Œè‡ªå®šä¹‰åŸŸçš„æ‰€æœ‰ç®—å­ Schema
void RegisterMyVirtualNpuSchemas();

}  // namespace contrib
}  // namespace onnxruntime
```

### 1.2 ç®—å­ Schema å®šä¹‰

Schema å®šä¹‰äº†ç®—å­çš„æ¥å£è§„èŒƒï¼ŒåŒ…æ‹¬è¾“å…¥è¾“å‡ºã€ç±»å‹çº¦æŸç­‰ï¼š

```cpp
// onnxruntime/core/providers/my_cpu/my_virtual_npu_defs.cc
#include <onnx/defs/schema.h>
#include "onnxruntime/core/graph/constants.h"

namespace onnxruntime {
namespace contrib {

static bool my_virtual_npu_schemas_registered = false;

void RegisterMyVirtualNpuSchemas() {
    // ä½¿ç”¨é™æ€æ ‡å¿—ä¿è¯åªæ³¨å†Œä¸€æ¬¡ï¼ˆå¹‚ç­‰æ€§ï¼‰
    if (my_virtual_npu_schemas_registered) {
        return;
    }

    // æ³¨å†Œè‡ªå®šä¹‰åŸŸ
    ONNX_NAMESPACE::OpSchemaRegistry::DomainToVersionRange domain_to_version;
    domain_to_version[kMyCustomDomain] = std::make_pair(1, 1);
    ONNX_NAMESPACE::RegisterSchema::Register(domain_to_version);

    // æ³¨å†Œ FastGelu ç®—å­ Schema
    ONNX_NAMESPACE::ONNX_OPERATOR_SET_SCHEMA_EX(
        FastGelu,
        kMyCustomDomain,
        1,
        false,  // ä¸å…è®¸é‡å¤æ³¨å†Œ
        OpSchema()
            .SetDoc("Fast Gaussian Error Linear Unit: y = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))")
            .Input(0, "X", "Input tensor", "T")
            .Output(0, "Y", "Output tensor", "T")
            .TypeConstraint(
                "T",
                {"tensor(float)", "tensor(float16)"},
                "Constrain input and output types to float tensors.")
            .TypeAndShapeInferenceFunction([](ONNX_NAMESPACE::InferenceContext& ctx) {
                propagateElemTypeFromInputToOutput(ctx, 0, 0);
                if (hasInputShape(ctx, 0)) {
                    propagateShapeFromInputToOutput(ctx, 0, 0);
                }
            }));

    my_virtual_npu_schemas_registered = true;
}

}  // namespace contrib
}  // namespace onnxruntime
```

**å…³é”®ç‚¹è¯´æ˜ï¼š**
- ä½¿ç”¨ `ONNX_OPERATOR_SET_SCHEMA_EX` å®æ³¨å†Œ Schemaï¼Œéœ€è¦ä¼ é€’ 6 ä¸ªå‚æ•°
- `false` å‚æ•°ç¡®ä¿ä¸å…è®¸é‡å¤æ³¨å†Œï¼Œé¿å…å†²çª
- `TypeAndShapeInferenceFunction` ç”¨äºç±»å‹å’Œå½¢çŠ¶æ¨å¯¼
- é™æ€æ ‡å¿— `my_virtual_npu_schemas_registered` ä¿è¯å¹‚ç­‰æ€§

### 1.3 Schema æ³¨å†Œæ—¶æœº

Schema å¿…é¡»åœ¨ ONNXRuntime åˆå§‹åŒ–æ—¶æ³¨å†Œï¼š

```cpp
// onnxruntime/core/session/environment.cc
#include "core/providers/my_cpu/my_virtual_npu_defs.h"

Status Environment::Create(std::unique_ptr<logging::LoggingManager> logging_manager,
                          std::unique_ptr<Environment>& environment,
                          const OrtThreadingOptions* tp_options,
                          bool create_global_thread_pools) {
    // ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...

    // æ³¨å†Œè‡ªå®šä¹‰åŸŸçš„ Schema
    contrib::RegisterMyVirtualNpuSchemas();

    // ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
}
```

## äºŒã€ç®—å­å†…æ ¸å®ç°

### 2.1 FastGelu æ•°å­¦åŸç†

FastGelu æ˜¯ GELU æ¿€æ´»å‡½æ•°çš„å¿«é€Ÿè¿‘ä¼¼å®ç°ï¼š

$$
\text{GELU}(x) = 0.5 \times x \times \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}} \times (x + 0.044715 \times x^3)\right)\right)
$$

è¯¥æ¿€æ´»å‡½æ•°åœ¨ GPTã€BERT ç­‰ Transformer æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨ã€‚

### 2.2 å†…æ ¸å®ç°ä»£ç 

```cpp
// onnxruntime/core/providers/my_cpu/nn/fast_gelu.h
#pragma once
#include "core/common/common.h"
#include "core/framework/op_kernel.h"

namespace onnxruntime {
namespace contrib {

template <typename T>
class FastGelu final : public OpKernel {
 public:
  FastGelu(const OpKernelInfo& info) : OpKernel(info) {}

  Status Compute(OpKernelContext* context) const override;
};

}  // namespace contrib
}  // namespace onnxruntime
```

```cpp
// onnxruntime/core/providers/my_cpu/nn/fast_gelu.cc
#include "fast_gelu.h"
#include "core/providers/cpu/nn/gelu_approximation.h"

namespace onnxruntime {
namespace contrib {

template <typename T>
Status FastGelu<T>::Compute(OpKernelContext* context) const {
  const Tensor* input = context->Input<Tensor>(0);
  const T* input_data = input->Data<T>();

  Tensor* output = context->Output(0, input->Shape());
  T* output_data = output->MutableData<T>();

  const auto& shape = input->Shape();
  int64_t total_elements = shape.Size();

  // ä½¿ç”¨æ ‡é‡è®¡ç®—ï¼ˆå¯ä¼˜åŒ–ä¸º SIMDï¼‰
  for (int64_t i = 0; i < total_elements; i++) {
    output_data[i] = ComputeGeluScalar(input_data[i]);
  }

  return Status::OK();
}

// æ˜¾å¼å®ä¾‹åŒ–
template class FastGelu<float>;

}  // namespace contrib
}  // namespace onnxruntime
```

### 2.3 å†…æ ¸æ³¨å†Œ

```cpp
// onnxruntime/core/providers/my_cpu/my_cpu_kernels.cc
#include "core/framework/op_kernel.h"
#include "nn/fast_gelu.h"
#include "my_virtual_npu_defs.h"

namespace onnxruntime {
namespace contrib {

// å®šä¹‰å†…æ ¸æ³¨å†Œå®
#define REGISTER_MY_CPU_KERNEL_TYPED(name, T, builder) \
  ONNX_OPERATOR_TYPED_KERNEL_EX(                        \
      name,                                             \
      kMyCustomDomain,                                  \
      1,                                                \
      T,                                                \
      kCpuExecutionProvider,                            \
      KernelDefBuilder().TypeConstraint("T", DataTypeImpl::GetTensorType<T>()), \
      builder)

void RegisterMyCpuKernels(KernelRegistry& kernel_registry) {
  static const BuildKernelCreateInfoFn function_table[] = {
      BuildKernelCreateInfo<REGISTER_MY_CPU_KERNEL_TYPED(FastGelu, float, FastGelu<float>)>,
      // å¯ä»¥æ·»åŠ æ›´å¤šç®—å­...
  };

  for (auto& function : function_table) {
    ORT_THROW_IF_ERROR(kernel_registry.Register(function()));
  }
}

}  // namespace contrib
}  // namespace onnxruntime
```

### 2.4 é›†æˆåˆ° CPU ExecutionProvider

```cpp
// onnxruntime/core/providers/cpu/cpu_execution_provider.cc
#include "core/providers/my_cpu/my_cpu_kernels.h"

namespace onnxruntime {

CPUExecutionProvider::CPUExecutionProvider(const CPUExecutionProviderInfo& info)
    : IExecutionProvider{onnxruntime::kCpuExecutionProvider, true} {
  // ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...

  // æ³¨å†Œè‡ªå®šä¹‰ my_cpu ç®—å­å†…æ ¸
  contrib::RegisterMyCpuKernels(*registry_);
}

}  // namespace onnxruntime
```

## ä¸‰ã€ç¼–è¯‘æ„å»º

### 3.1 CMake é…ç½®

```cmake
# onnxruntime/core/providers/my_cpu/CMakeLists.txt
set(my_cpu_sources
  my_virtual_npu_defs.cc
  my_cpu_kernels.cc
  nn/fast_gelu.cc
)

add_library(onnxruntime_providers_my_cpu OBJECT ${my_cpu_sources})
target_include_directories(onnxruntime_providers_my_cpu PRIVATE
  ${ONNXRUNTIME_ROOT}
  ${ONNXRUNTIME_ROOT}/core
)
```

### 3.2 ç¼–è¯‘å‘½ä»¤

```bash
# é…ç½®æ„å»º
./build.sh --config Release \
  --parallel \
  --skip_submodule_sync \
  --skip_tests \
  --build_shared_lib

# ç¼–è¯‘å®Œæˆåï¼Œåº“æ–‡ä»¶ä½äº
# build/Linux/Release/libonnxruntime.so
```

### 3.3 ç¼–è¯‘ä¼˜åŒ–å»ºè®®

- ä½¿ç”¨ `--parallel` åŠ é€Ÿç¼–è¯‘
- å¼€å‘é˜¶æ®µä½¿ç”¨ `--config Debug` ä¾¿äºè°ƒè¯•
- ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ `--config Release` è·å¾—æœ€ä½³æ€§èƒ½
- æ·»åŠ  `--enable_pybind` æ”¯æŒ Python ç»‘å®š

## å››ã€å•å…ƒæµ‹è¯•

### 4.1 æµ‹è¯•æ¡†æ¶

ONNXRuntime ä½¿ç”¨ Google Test æ¡†æ¶ï¼Œæä¾›äº† `OpTester` å·¥å…·ç±»ç®€åŒ–ç®—å­æµ‹è¯•ï¼š

```cpp
// onnxruntime/test/providers/my_cpu/nn/fast_gelu_op_test.cc
#include "gtest/gtest.h"
#include "test/providers/provider_test_utils.h"
#include "core/providers/my_cpu/my_virtual_npu_defs.h"

namespace onnxruntime {
namespace test {

// ç¡®ä¿ Schema å·²æ³¨å†Œ
static void EnsureSchemasRegistered() {
    static bool initialized = false;
    if (!initialized) {
        contrib::RegisterMyVirtualNpuSchemas();
        initialized = true;
    }
}

TEST(FastGeluTest, Basic) {
    EnsureSchemasRegistered();

    OpTester test("FastGelu", 1, contrib::kMyCustomDomain);

    // è¾“å…¥æ•°æ®
    std::vector<int64_t> dims{2, 3};
    std::vector<float> input_data = {-1.0f, 0.0f, 1.0f, 2.0f, -2.0f, 0.5f};

    // æœŸæœ›è¾“å‡ºï¼ˆæ ¹æ® GELU å…¬å¼è®¡ç®—ï¼‰
    std::vector<float> expected_output = {
        -0.15865529f,  // GELU(-1.0)
        0.0f,          // GELU(0.0)
        0.8413447f,    // GELU(1.0)
        1.9545977f,    // GELU(2.0)
        -0.04540223f,  // GELU(-2.0)
        0.34571534f    // GELU(0.5)
    };

    test.AddInput<float>("X", dims, input_data);
    test.AddOutput<float>("Y", dims, expected_output);

    // è¿è¡Œæµ‹è¯•ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨ CPU ExecutionProviderï¼‰
    test.Run();
}

TEST(FastGeluTest, LargeInput) {
    EnsureSchemasRegistered();

    OpTester test("FastGelu", 1, contrib::kMyCustomDomain);

    // æµ‹è¯•å¤§å¼ é‡
    std::vector<int64_t> dims{128, 768};  // ç±»ä¼¼ BERT hidden size
    int64_t total = 128 * 768;

    std::vector<float> input_data(total);
    std::vector<float> expected_output(total);

    // ç”Ÿæˆæµ‹è¯•æ•°æ®
    for (int64_t i = 0; i < total; i++) {
        float x = (i % 100 - 50) * 0.1f;  // èŒƒå›´ [-5, 5]
        input_data[i] = x;

        // è®¡ç®—æœŸæœ›è¾“å‡º
        float x3 = x * x * x;
        float inner = 0.7978845608f * (x + 0.044715f * x3);
        expected_output[i] = 0.5f * x * (1.0f + std::tanh(inner));
    }

    test.AddInput<float>("X", dims, input_data);
    test.AddOutput<float>("Y", dims, expected_output);
    test.Run();
}

TEST(FastGeluTest, EdgeCases) {
    EnsureSchemasRegistered();

    OpTester test("FastGelu", 1, contrib::kMyCustomDomain);

    // æµ‹è¯•è¾¹ç•Œæƒ…å†µ
    std::vector<int64_t> dims{6};
    std::vector<float> input_data = {
        -10.0f,   // æå°å€¼
        10.0f,    // æå¤§å€¼
        0.0f,     // é›¶
        -0.0f,    // è´Ÿé›¶
        1e-7f,    // æ¥è¿‘é›¶
        -1e-7f    // æ¥è¿‘è´Ÿé›¶
    };

    std::vector<float> expected_output(6);
    for (size_t i = 0; i < input_data.size(); i++) {
        float x = input_data[i];
        float x3 = x * x * x;
        float inner = 0.7978845608f * (x + 0.044715f * x3);
        expected_output[i] = 0.5f * x * (1.0f + std::tanh(inner));
    }

    test.AddInput<float>("X", dims, input_data);
    test.AddOutput<float>("Y", dims, expected_output);
    test.Run();
}

}  // namespace test
}  // namespace onnxruntime
```

### 4.2 è¿è¡Œæµ‹è¯•

```bash
# ç¼–è¯‘æµ‹è¯•
./build.sh --config Release --build_shared_lib --enable_tests

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./build/Linux/Release/onnxruntime_test_all

# è¿è¡Œç‰¹å®šæµ‹è¯•å¥—ä»¶
./build/Linux/Release/onnxruntime_test_all --gtest_filter="FastGeluTest.*"

# è¿è¡Œå•ä¸ªæµ‹è¯•
./build/Linux/Release/onnxruntime_test_all --gtest_filter="FastGeluTest.Basic"
```

### 4.3 æµ‹è¯•è¾“å‡ºç¤ºä¾‹

```
[==========] Running 3 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 3 tests from FastGeluTest
[ RUN      ] FastGeluTest.Basic
[       OK ] FastGeluTest.Basic (12 ms)
[ RUN      ] FastGeluTest.LargeInput
[       OK ] FastGeluTest.LargeInput (156 ms)
[ RUN      ] FastGeluTest.EdgeCases
[       OK ] FastGeluTest.EdgeCases (8 ms)
[----------] 3 tests from FastGeluTest (176 ms total)

[==========] 3 tests from 1 test suite ran. (176 ms total)
[  PASSED  ] 3 tests.
```

## äº”ã€Python é›†æˆä¸å¤§æ¨¡å‹æµ‹è¯•

### 5.1 Python åŒ…å®‰è£…

```bash
# æ–¹å¼ 1: ä»æºç æ„å»ºå®‰è£…
python setup.py install

# æ–¹å¼ 2: æ„å»º wheel åŒ…
python tools/ci_build/build.py --build_wheel --config Release
pip install build/Linux/Release/dist/onnxruntime-1.20.0-*.whl

# æ–¹å¼ 3: å¼€å‘æ¨¡å¼å®‰è£…
pip install -e . --no-build-isolation
```

### 5.2 éªŒè¯å®‰è£…

```python
# check_onnxruntime_version.py
import onnxruntime as ort
import numpy as np

print(f"ONNXRuntime ç‰ˆæœ¬: {ort.__version__}")
print(f"å¯ç”¨çš„ Execution Providers: {ort.get_available_providers()}")

# æµ‹è¯•è‡ªå®šä¹‰ç®—å­
def test_custom_fastgelu():
    """æµ‹è¯•è‡ªå®šä¹‰ FastGelu ç®—å­"""
    # åˆ›å»ºç®€å•çš„ ONNX æ¨¡å‹ï¼ˆåŒ…å« FastGeluï¼‰
    import onnx
    from onnx import helper, TensorProto

    # å®šä¹‰è¾“å…¥è¾“å‡º
    input_tensor = helper.make_tensor_value_info('X', TensorProto.FLOAT, [2, 3])
    output_tensor = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [2, 3])

    # åˆ›å»º FastGelu èŠ‚ç‚¹
    fastgelu_node = helper.make_node(
        'FastGelu',
        inputs=['X'],
        outputs=['Y'],
        domain='com.my_virtual_npu'
    )

    # åˆ›å»ºå›¾
    graph = helper.make_graph(
        [fastgelu_node],
        'test_fastgelu',
        [input_tensor],
        [output_tensor]
    )

    # åˆ›å»ºæ¨¡å‹
    model = helper.make_model(graph, producer_name='test')

    # ä¿å­˜æ¨¡å‹
    onnx.save(model, 'test_fastgelu.onnx')

    # è¿è¡Œæ¨ç†
    session = ort.InferenceSession('test_fastgelu.onnx')

    input_data = np.array([[-1.0, 0.0, 1.0], [2.0, -2.0, 0.5]], dtype=np.float32)
    outputs = session.run(None, {'X': input_data})

    print("è¾“å…¥:", input_data)
    print("è¾“å‡º:", outputs[0])
    print("è‡ªå®šä¹‰ç®—å­æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_custom_fastgelu()
```

### 5.3 å¤§æ¨¡å‹æµ‹è¯•ï¼šTiny-GPT2

```python
# test_tiny_gpt2.py
import onnxruntime as ort
import numpy as np
from transformers import GPT2Tokenizer
import time

def test_tiny_gpt2():
    """æµ‹è¯• Tiny-GPT2 æ¨¡å‹ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ FastGelu ç®—å­ï¼‰"""

    # åŠ è½½æ¨¡å‹ï¼ˆå‡è®¾å·²å°† Gelu æ›¿æ¢ä¸º FastGeluï¼‰
    model_path = "models/tiny-gpt2-fastgelu.onnx"

    # åˆ›å»ºä¼šè¯
    session_options = ort.SessionOptions()
    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    session = ort.InferenceSession(
        model_path,
        sess_options=session_options,
        providers=['CPUExecutionProvider']
    )

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print("=" * 60)
    print("æ¨¡å‹è¾“å…¥:")
    for input_meta in session.get_inputs():
        print(f"  {input_meta.name}: {input_meta.shape} ({input_meta.type})")

    print("\næ¨¡å‹è¾“å‡º:")
    for output_meta in session.get_outputs():
        print(f"  {output_meta.name}: {output_meta.shape} ({output_meta.type})")
    print("=" * 60)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    text = "Hello, I am a language model"

    # Tokenize
    inputs = tokenizer(text, return_tensors="np")
    input_ids = inputs["input_ids"].astype(np.int64)
    attention_mask = inputs["attention_mask"].astype(np.int64)

    print(f"\nè¾“å…¥æ–‡æœ¬: {text}")
    print(f"Input IDs shape: {input_ids.shape}")

    # æ¨ç†æ€§èƒ½æµ‹è¯•
    warmup_runs = 3
    test_runs = 10

    print(f"\né¢„çƒ­è¿è¡Œ {warmup_runs} æ¬¡...")
    for _ in range(warmup_runs):
        session.run(None, {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        })

    print(f"æ€§èƒ½æµ‹è¯• {test_runs} æ¬¡...")
    times = []
    for i in range(test_runs):
        start_time = time.perf_counter()
        outputs = session.run(None, {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        })
        end_time = time.perf_counter()
        times.append(end_time - start_time)

    # ç»Ÿè®¡ç»“æœ
    avg_time = np.mean(times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    std_time = np.std(times) * 1000
    min_time = np.min(times) * 1000
    max_time = np.max(times) * 1000

    print("\n" + "=" * 60)
    print("æ€§èƒ½ç»Ÿè®¡:")
    print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
    print(f"  æ ‡å‡†å·®:       {std_time:.2f} ms")
    print(f"  æœ€å°å€¼:       {min_time:.2f} ms")
    print(f"  æœ€å¤§å€¼:       {max_time:.2f} ms")
    print(f"  ååé‡:       {1000/avg_time:.2f} samples/sec")
    print("=" * 60)

    # è¾“å‡ºé¢„æµ‹ç»“æœ
    logits = outputs[0]
    print(f"\nLogits shape: {logits.shape}")
    print(f"Logits ç»Ÿè®¡: min={logits.min():.4f}, max={logits.max():.4f}, mean={logits.mean():.4f}")

    # ç”Ÿæˆæ–‡æœ¬ï¼ˆç®€å•è´ªå©ªè§£ç ï¼‰
    next_token_id = np.argmax(logits[0, -1, :])
    next_token = tokenizer.decode([next_token_id])
    print(f"\né¢„æµ‹çš„ä¸‹ä¸€ä¸ª token: '{next_token}' (ID: {next_token_id})")

    print("\nâœ… å¤§æ¨¡å‹æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_tiny_gpt2()
```

### 5.4 æ€§èƒ½å¯¹æ¯”

```python
# benchmark_fastgelu.py
import onnxruntime as ort
import numpy as np
import time

def benchmark_comparison():
    """å¯¹æ¯”æ ‡å‡† Gelu å’Œè‡ªå®šä¹‰ FastGelu çš„æ€§èƒ½"""

    # æµ‹è¯•é…ç½®
    batch_size = 32
    seq_length = 128
    hidden_size = 768
    iterations = 100

    input_shape = (batch_size, seq_length, hidden_size)
    input_data = np.random.randn(*input_shape).astype(np.float32)

    # æµ‹è¯•æ ‡å‡† Gelu
    print("æµ‹è¯•æ ‡å‡† Gelu...")
    # ... (çœç•¥æ¨¡å‹åˆ›å»ºä»£ç )

    # æµ‹è¯•è‡ªå®šä¹‰ FastGelu
    print("æµ‹è¯•è‡ªå®šä¹‰ FastGelu...")
    # ... (çœç•¥æ¨¡å‹åˆ›å»ºä»£ç )

    print("\næ€§èƒ½å¯¹æ¯”:")
    print(f"æ ‡å‡† Gelu:    {gelu_time:.2f} ms")
    print(f"è‡ªå®šä¹‰ FastGelu: {fastgelu_time:.2f} ms")
    print(f"åŠ é€Ÿæ¯”:       {gelu_time/fastgelu_time:.2f}x")

if __name__ == "__main__":
    benchmark_comparison()
```

## å…­ã€C++ SDK æ‰“åŒ…ä¸åˆ†å‘

### 6.1 æ‰“åŒ…è„šæœ¬

ä¸ºäº†æ–¹ä¾¿å…¶ä»–å¼€å‘è€…ä½¿ç”¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†è‡ªåŠ¨æ‰“åŒ…è„šæœ¬ï¼š

```bash
#!/bin/bash
# pre_package_cpp_sdk.sh
# å°†ç¼–è¯‘å¥½çš„åº“ã€å¤´æ–‡ä»¶æ‰“åŒ…åˆ° PreRelease ç›®å½•

set -e

echo "ğŸ“¦ æ‰“åŒ… ONNXRuntime C++ SDK..."

# é…ç½®
BUILD_DIR="${1:-build/Linux/Release}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
RELEASE_DIR="PreRelease_${TIMESTAMP}/cpp"
VERSION="1.20.0-custom"

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p "$RELEASE_DIR"/{include,lib,bin,examples}

# å¤åˆ¶å¤´æ–‡ä»¶
echo "ğŸ“‹ å¤åˆ¶å¤´æ–‡ä»¶..."
cp -r include/onnxruntime "$RELEASE_DIR/include/"

# å¤åˆ¶åº“æ–‡ä»¶
echo "ğŸ“š å¤åˆ¶åº“æ–‡ä»¶..."
cp "$BUILD_DIR"/libonnxruntime.so* "$RELEASE_DIR/lib/"

# åˆ›å»º CMake é…ç½®
cat > "$RELEASE_DIR/ONNXRuntimeConfig.cmake" << 'EOF'
get_filename_component(ONNXRUNTIME_CMAKE_DIR "${CMAKE_CURRENT_LIST_FILE}" PATH)
set(ONNXRUNTIME_INCLUDE_DIRS "${ONNXRUNTIME_CMAKE_DIR}/include")
set(ONNXRUNTIME_LIBRARIES "${ONNXRUNTIME_CMAKE_DIR}/lib/libonnxruntime.so")

add_library(onnxruntime SHARED IMPORTED)
set_target_properties(onnxruntime PROPERTIES
    IMPORTED_LOCATION "${ONNXRUNTIME_LIBRARIES}"
    INTERFACE_INCLUDE_DIRECTORIES "${ONNXRUNTIME_INCLUDE_DIRS}"
)
EOF

# åˆ›å»ºç¤ºä¾‹ä»£ç 
cat > "$RELEASE_DIR/examples/simple_inference.cpp" << 'EOF'
#include <onnxruntime/core/session/onnxruntime_cxx_api.h>
#include <iostream>

int main() {
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "test");
    Ort::SessionOptions session_options;
    Ort::Session session(env, "model.onnx", session_options);

    std::cout << "è¾“å…¥èŠ‚ç‚¹æ•°: " << session.GetInputCount() << std::endl;
    std::cout << "è¾“å‡ºèŠ‚ç‚¹æ•°: " << session.GetOutputCount() << std::endl;

    return 0;
}
EOF

echo "âœ… æ‰“åŒ…å®Œæˆ: $RELEASE_DIR"
```

### 6.2 ä½¿ç”¨æ‰“åŒ…çš„ SDK

```cmake
# ç”¨æˆ·çš„ CMakeLists.txt
cmake_minimum_required(VERSION 3.13)
project(MyApp)

set(CMAKE_CXX_STANDARD 17)

# æ‰¾åˆ° ONNXRuntime
set(ONNXRuntime_DIR /path/to/PreRelease_xxx/cpp)
find_package(ONNXRuntime REQUIRED)

# åˆ›å»ºåº”ç”¨
add_executable(my_app main.cpp)
target_link_libraries(my_app onnxruntime)
```

## ä¸ƒã€å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 7.1 Schema æ³¨å†Œé—®é¢˜

**é—®é¢˜ï¼š** `No Schema registered for 'FastGelu'!`

**åŸå› ï¼š** Schema æœªåœ¨æ¨¡å‹åŠ è½½å‰æ³¨å†Œ

**è§£å†³æ–¹æ¡ˆï¼š**
```cpp
// åœ¨ Environment::Create() ä¸­æ·»åŠ 
contrib::RegisterMyVirtualNpuSchemas();
```

### 7.2 Domain å†²çª

**é—®é¢˜ï¼š** ä¸ Microsoft å†…ç½®ç®—å­å†²çª

**è§£å†³æ–¹æ¡ˆï¼š** ä½¿ç”¨è‡ªå®šä¹‰åŸŸå
```cpp
constexpr const char* kMyCustomDomain = "com.my_virtual_npu";
```

### 7.3 Kernel æœªæ‰¾åˆ°

**é—®é¢˜ï¼š** `Kernel not found: FastGelu`

**åŸå› ï¼š** Kernel æœªæ³¨å†Œåˆ° ExecutionProvider

**è§£å†³æ–¹æ¡ˆï¼š**
```cpp
// åœ¨ CPUExecutionProvider æ„é€ å‡½æ•°ä¸­
contrib::RegisterMyCpuKernels(*registry_);
```

### 7.4 è¾“å…¥è¾“å‡ºä¸åŒ¹é…

**é—®é¢˜ï¼š** `Input count mismatch: expected 2, got 1`

**åŸå› ï¼š** Kernel å®ç°ä¸ Schema å®šä¹‰ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆï¼š** ç¡®ä¿ Schema å’Œ Kernel çš„è¾“å…¥è¾“å‡ºæ•°é‡ã€ç±»å‹å®Œå…¨ä¸€è‡´

### 7.5 Python åŒ…å®‰è£…å¤±è´¥

**é—®é¢˜ï¼š** `No such file or directory: 'build/Linux/Release/wheel'`

**åŸå› ï¼š** æœªæ­£ç¡®ç”Ÿæˆ wheel åŒ…

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ä½¿ç”¨ CI æ„å»ºå·¥å…·
python tools/ci_build/build.py --build_wheel --config Release
```

## å…«ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 8.1 SIMD ä¼˜åŒ–

```cpp
// ä½¿ç”¨ AVX2 åŠ é€Ÿ
#include <immintrin.h>

void FastGeluAVX2(const float* input, float* output, int64_t size) {
    for (int64_t i = 0; i < size; i += 8) {
        __m256 x = _mm256_loadu_ps(input + i);
        // ... AVX2 GELU è®¡ç®— ...
        _mm256_storeu_ps(output + i, result);
    }
}
```

### 8.2 å¤šçº¿ç¨‹å¹¶è¡Œ

```cpp
#include "core/platform/threadpool.h"

// ä½¿ç”¨ ONNXRuntime çš„çº¿ç¨‹æ± 
context->GetOperatorThreadPool()->ParallelFor(
    total_elements,
    [&](std::ptrdiff_t i) {
        output_data[i] = ComputeGeluScalar(input_data[i]);
    }
);
```

### 8.3 å†…å­˜ä¼˜åŒ–

- ä½¿ç”¨ `MutableData()` è€Œé `Data()` é¿å…ä¸å¿…è¦çš„æ‹·è´
- åˆ©ç”¨ `AllocatorPtr` ç®¡ç†å¤§å—å†…å­˜
- è€ƒè™‘ä½¿ç”¨ Arena åˆ†é…å™¨å‡å°‘å†…å­˜ç¢ç‰‡

## ä¹ã€æ€»ç»“

æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†åœ¨ ONNXRuntime ä¸­å¼€å‘è‡ªå®šä¹‰ç®—å­çš„å®Œæ•´æµç¨‹ï¼š

1. **æ¶æ„è®¾è®¡**ï¼šè‡ªå®šä¹‰åŸŸã€Schema æ³¨å†Œã€Kernel æ³¨å†Œ
2. **ç®—å­å®ç°**ï¼šFastGelu çš„æ•°å­¦åŸç†å’Œ C++ å®ç°
3. **ç¼–è¯‘æ„å»º**ï¼šCMake é…ç½®ã€ç¼–è¯‘é€‰é¡¹
4. **å•å…ƒæµ‹è¯•**ï¼šä½¿ç”¨ OpTester è¿›è¡Œå…¨é¢æµ‹è¯•
5. **Python é›†æˆ**ï¼šå®‰è£…ã€éªŒè¯ã€å¤§æ¨¡å‹æµ‹è¯•
6. **SDK æ‰“åŒ…**ï¼šæ–¹ä¾¿åˆ†å‘å’Œä½¿ç”¨

é€šè¿‡æœ¬æ–‡çš„å®è·µï¼Œæ‚¨å¯ä»¥ï¼š
- ç†è§£ ONNXRuntime çš„ç®—å­æ³¨å†Œæœºåˆ¶
- æŒæ¡è‡ªå®šä¹‰ç®—å­çš„å¼€å‘æµç¨‹
- å­¦ä¼šå•å…ƒæµ‹è¯•å’Œæ€§èƒ½éªŒè¯
- èƒ½å¤Ÿå°†è‡ªå®šä¹‰ç®—å­é›†æˆåˆ°å®é™…é¡¹ç›®ä¸­

## å‚è€ƒèµ„æ–™

- [ONNXRuntime å®˜æ–¹æ–‡æ¡£](https://onnxruntime.ai/docs/)
- [ONNX Operator Schemas](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
- [ONNXRuntime è‡ªå®šä¹‰ç®—å­æŒ‡å—](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html)
- [GELU è®ºæ–‡](https://arxiv.org/abs/1606.08415)

---

**ä½œè€…ä¿¡æ¯**
- GitHub: [onnxruntime_my_virtual_npu](https://github.com/Han-Zhenzhong/onnxruntime_my_virtual_npu)
- ç‰ˆæœ¬: ONNXRuntime 1.20.0
- æœ€åæ›´æ–°: 2025-11-19

**è®¸å¯è¯**
æœ¬æ–‡æ¡£éµå¾ª MIT Licenseï¼Œæ¬¢è¿è½¬è½½å’Œä¿®æ”¹ï¼Œä½†è¯·ä¿ç•™åŸä½œè€…ä¿¡æ¯ã€‚
