# ONNX Runtime è‡ªå®šä¹‰ç®—å­å®ç°è®¡åˆ’ - Tiny-GPT2 CPU åŸºç¡€ç‰ˆ

## ğŸ¯ å®ç°çŠ¶æ€æ€»è§ˆ

**æœ€åæ›´æ–°**: 2025-11-18

### âœ… å·²å®Œæˆï¼ˆé˜¶æ®µ1 åŸºç¡€å®ç°ï¼‰
- [x] ç›®å½•ç»“æ„åˆ›å»º (`my_cpu/`, `test/my_cpu/`)
- [x] **FastGELU ç®—å­å®ç°**ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼Œæ ‡é‡å®ç°ï¼‰
  - æ–‡ä»¶ï¼š`my_cpu/bert/fast_gelu.{h,cc}` (~200 è¡Œ)
  - ç‰¹æ€§ï¼šæ”¯æŒ bias è¾“å…¥ï¼Œå®Œæ•´é”™è¯¯å¤„ç†
  - ä¼˜åŒ–ï¼šåŒ…å« TODO-OPTIMIZE æ ‡æ³¨ï¼ˆAVX2, OpenMPï¼‰
- [x] **FastGELU å•å…ƒæµ‹è¯•**ï¼ˆå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼‰
  - æ–‡ä»¶ï¼š`test/my_cpu/fast_gelu_op_test.cc` (~180 è¡Œ)
  - è¦†ç›–ï¼š5ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…å«è¾¹ç•Œæƒ…å†µå’Œå¤§å¼ é‡
- [x] **ç®—å­æ³¨å†Œç³»ç»Ÿ**
  - æ–‡ä»¶ï¼š`my_cpu/my_cpu_kernels.{h,cc}` (~60 è¡Œ)
  - çŠ¶æ€ï¼šFastGelu å·²æ³¨å†Œåˆ° kMSDomain
- [x] **CMake æ„å»ºé…ç½®**
  - æ–‡ä»¶ï¼š`my_cpu/CMakeLists.txt`, `test/my_cpu/CMakeLists.txt`
  - ç‰¹æ€§ï¼šç‹¬ç«‹æ„å»ºï¼Œé¢„ç•™ AVX2 ç¼–è¯‘é€‰é¡¹
- [x] **å®Œæ•´æ–‡æ¡£**ï¼ˆ~900 è¡Œï¼‰
  - README.md - ä½¿ç”¨æŒ‡å—
  - INTEGRATION.md - é›†æˆæ­¥éª¤
  - QUICKSTART.md - å¿«é€Ÿå‚è€ƒ
- [x] **è¾…åŠ©å·¥å…·**
  - generate_test_data.py - æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
  - verify.sh / verify.bat - éªŒè¯è„šæœ¬

**å®Œæˆåº¦**: åŸºç¡€å®ç° 100% âœ…

### ğŸ”„ è¿›è¡Œä¸­
- æ— 

### â­ï¸ å¾…å®Œæˆ
- [ ] ç¼–è¯‘å’Œå•å…ƒæµ‹è¯•éªŒè¯
- [ ] LayerNormalization éªŒè¯/å®ç°
- [ ] Attention éªŒè¯/å®ç°
- [ ] Tiny-GPT2 ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆSIMD, OpenMPï¼‰

### ğŸ“Š è¿›åº¦æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | çŠ¶æ€ |
|------|--------|--------|------|
| FastGELU å®ç° | 100% | 100% | âœ… å®Œæˆ |
| å•å…ƒæµ‹è¯•è¦†ç›– | 100% | 100% | âœ… å®Œæˆ |
| æ–‡æ¡£å®Œæ•´åº¦ | 100% | 100% | âœ… å®Œæˆ |
| é›†æˆåˆ°æ„å»ºç³»ç»Ÿ | 100% | 100% | âœ… å®Œæˆ |
| ç¼–è¯‘éªŒè¯ | 0% | 100% | â­ï¸ å¾…å®Œæˆ |
| ç«¯åˆ°ç«¯æµ‹è¯• | 0% | 100% | â­ï¸ å¾…å®Œæˆ |

---

## ğŸ“‹ æ›´æ–°æ—¥å¿—

### 2025-11-18 - é›†æˆåˆ°ä¸»æ„å»ºç³»ç»Ÿ âœ…

**é›†æˆå®Œæˆ**ï¼š
- âœ… ä¿®æ”¹ `cmake/onnxruntime_providers_cpu.cmake`
  - æ·»åŠ  my_cpu æºæ–‡ä»¶æ‰«æ
  - å°† my_cpu æºæ–‡ä»¶åŠ å…¥ onnxruntime_providers åº“
- âœ… ä¿®æ”¹ `onnxruntime/core/providers/cpu/cpu_execution_provider.cc`
  - åŒ…å« my_cpu/my_cpu_kernels.h å¤´æ–‡ä»¶
  - åœ¨ RegisterCPUKernels() ä¸­è°ƒç”¨ RegisterMyCpuKernels()
- âœ… ä¿®æ”¹ `cmake/onnxruntime_unittests.cmake`
  - æ·»åŠ  test/my_cpu æµ‹è¯•æºæ–‡ä»¶æ‰«æ
  - å°†æµ‹è¯•æ–‡ä»¶åŠ å…¥ onnxruntime_test_all æµ‹è¯•å¥—ä»¶

**é›†æˆæ–¹å¼**ï¼š
- my_cpu ç®—å­ä¸ contrib_ops ç±»ä¼¼ï¼Œè¢«ç¼–è¯‘åˆ° onnxruntime_providers é™æ€åº“ä¸­
- CPU Execution Provider åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨æ³¨å†Œ my_cpu ç®—å­
- my_cpu æµ‹è¯•ç”¨ä¾‹è‡ªåŠ¨åŒ…å«åœ¨å•å…ƒæµ‹è¯•ä¸­

**ä¸‹ä¸€æ­¥**ï¼šç¼–è¯‘å¹¶è¿è¡Œå•å…ƒæµ‹è¯•éªŒè¯é›†æˆç»“æœ

### 2025-11-18 - åŸºç¡€å®ç°å®Œæˆ âœ…

**æ–°å¢å†…å®¹**ï¼š
- âœ… å®ç° FastGELU ç®—å­ï¼ˆæ ‡é‡ç‰ˆæœ¬ï¼‰
- âœ… å®Œæ•´å•å…ƒæµ‹è¯•å¥—ä»¶ï¼ˆ5ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
- âœ… ç®—å­æ³¨å†Œç³»ç»Ÿ
- âœ… CMake æ„å»ºé…ç½®
- âœ… å®Œæ•´æ–‡æ¡£ï¼ˆREADME, INTEGRATION, QUICKSTARTï¼‰
- âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå·¥å…·
- âœ… éªŒè¯è„šæœ¬

**æ–‡ä»¶åˆ›å»º**ï¼š
- `my_cpu/bert/fast_gelu.{h,cc}` - æ ¸å¿ƒå®ç°
- `my_cpu/my_cpu_kernels.{h,cc}` - æ³¨å†Œç³»ç»Ÿ
- `test/my_cpu/fast_gelu_op_test.cc` - å•å…ƒæµ‹è¯•
- `my_cpu/CMakeLists.txt` - æ„å»ºé…ç½®
- `my_cpu/{README,INTEGRATION,QUICKSTART}.md` - æ–‡æ¡£
- `my_cpu/generate_test_data.py` - å·¥å…·
- `my_cpu/verify.{sh,bat}` - éªŒè¯è„šæœ¬

**ä»£ç ç»Ÿè®¡**ï¼š
- æ ¸å¿ƒä»£ç ï¼š~400 è¡Œ
- æµ‹è¯•ä»£ç ï¼š~200 è¡Œ
- æ–‡æ¡£ï¼š~900 è¡Œ
- æ€»è®¡ï¼š~1,500 è¡Œ

**ä¸‹ä¸€æ­¥**ï¼šé›†æˆåˆ°ä¸»æ„å»ºç³»ç»Ÿå¹¶ç¼–è¯‘éªŒè¯

---

## ğŸ“‹ å®ç°æ–¹å¼è¯´æ˜

**ç›®å½•ç»“æ„**ï¼šæ‰€æœ‰å®ç°ä»£ç æ”¾åœ¨ `onnxruntime/my_cpu/` ç›®å½•ä¸‹ï¼Œç‹¬ç«‹äºç°æœ‰çš„ `contrib_ops/cpu/` ç›®å½•ã€‚

**ä¼˜åŠ¿**ï¼š
- âœ… ä¸ç°æœ‰ä»£ç å®Œå…¨éš”ç¦»ï¼Œäº’ä¸å½±å“
- âœ… ç‹¬ç«‹çš„å‘½åç©ºé—´ `onnxruntime::my_cpu`
- âœ… ä¾¿äºå­¦ä¹ ã€å®éªŒå’Œç»´æŠ¤
- âœ… å¯å‚è€ƒ contrib_ops å®ç°ï¼Œä½†ä¸ä¾èµ–å®ƒ

---

## 1. é¡¹ç›®æ¦‚è¿°

### å¿«é€Ÿå¼€å§‹

**ğŸ“Œ å½“å‰çŠ¶æ€ï¼šåŸºç¡€å®ç°å·²å®Œæˆï¼Œç­‰å¾…é›†æˆæµ‹è¯•**

```bash
# 1. âœ… å·²å®Œæˆï¼šç›®å½•ç»“æ„å·²åˆ›å»º
cd onnxruntime
# my_cpu/bert/ å’Œ test/my_cpu/ å·²åˆ›å»º

# 2. âœ… å·²å®Œæˆï¼šåŸºç¡€æ–‡ä»¶å·²å®ç°
# - my_cpu/my_cpu_kernels.h (å·²å®ç°)
# - my_cpu/my_cpu_kernels.cc (å·²å®ç°)
# - my_cpu/bert/fast_gelu.h (å·²å®ç°)
# - my_cpu/bert/fast_gelu.cc (å·²å®ç°)
# - my_cpu/CMakeLists.txt (å·²å®ç°)

# 3. âœ… å·²å®Œæˆï¼šæµ‹è¯•æ–‡ä»¶å·²å®ç°
# - test/my_cpu/fast_gelu_op_test.cc (å·²å®ç°)
# - test/my_cpu/CMakeLists.txt (å·²å®ç°)

# 4. â­ï¸ å¾…å®Œæˆï¼šç¼–è¯‘ï¼ˆéœ€è¦é›†æˆåˆ°ä¸»æ„å»ºç³»ç»Ÿï¼‰
./build.sh --config Release --parallel
```

**å·²å®ç°çš„æ–‡ä»¶æ¸…å•**ï¼š
- âœ… `my_cpu/bert/fast_gelu.h` - FastGELU å¤´æ–‡ä»¶
- âœ… `my_cpu/bert/fast_gelu.cc` - FastGELU å®ç°ï¼ˆæ ‡é‡ç‰ˆæœ¬ï¼Œå«ä¼˜åŒ–æ ‡æ³¨ï¼‰
- âœ… `my_cpu/my_cpu_kernels.h` - ç®—å­æ³¨å†Œå¤´æ–‡ä»¶
- âœ… `my_cpu/my_cpu_kernels.cc` - ç®—å­æ³¨å†Œå®ç°
- âœ… `my_cpu/CMakeLists.txt` - æ„å»ºé…ç½®
- âœ… `my_cpu/README.md` - ä½¿ç”¨æ–‡æ¡£
- âœ… `my_cpu/INTEGRATION.md` - é›†æˆæŒ‡å—
- âœ… `my_cpu/QUICKSTART.md` - å¿«é€Ÿå‚è€ƒ
- âœ… `my_cpu/generate_test_data.py` - æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
- âœ… `my_cpu/verify.sh` / `verify.bat` - éªŒè¯è„šæœ¬
- âœ… `test/my_cpu/fast_gelu_op_test.cc` - å•å…ƒæµ‹è¯•
- âœ… `test/my_cpu/CMakeLists.txt` - æµ‹è¯•æ„å»ºé…ç½®

æœ¬æ–‡æ¡£æè¿°äº†åœ¨ ONNX Runtime ä¸­ä¸º **Tiny-GPT2-ONNX** æ¨¡å‹å®ç° CPU ç®—å­çš„å®Œæ•´è®¡åˆ’ã€‚

**å¼€å‘ç­–ç•¥**ï¼šå…ˆå®ç°èƒ½æ­£ç¡®è¿è¡Œçš„åŸºç¡€ç‰ˆæœ¬ï¼Œåœ¨ä»£ç ä¸­æ ‡æ³¨ä¼˜åŒ–ç‚¹ï¼Œåç»­æŒ‰éœ€ä¼˜åŒ–ã€‚

### 1.1 ç›®æ ‡
- âœ… **é¦–è¦ç›®æ ‡**ï¼šå®ç°èƒ½æ­£ç¡®è¿è¡Œ Tiny-GPT2 çš„åŸºç¡€ç®—å­
- âœ… **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒå®Œæ•´çš„æ¨ç†æµç¨‹ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰
- âœ… **ç²¾åº¦ä¿è¯**ï¼šè¾“å‡ºç»“æœä¸åŸæ¨¡å‹ä¸€è‡´ï¼ˆè¯¯å·® < 1e-3ï¼‰
- ğŸ“ **ä¼˜åŒ–é¢„ç•™**ï¼šåœ¨ä»£ç ä¸­æ ‡æ³¨å¯ä¼˜åŒ–çš„ä½ç½®
- ğŸ“š **æ–‡æ¡£å®Œå–„**ï¼šæä¾›æ¸…æ™°çš„å®ç°è¯´æ˜å’Œæµ‹è¯•ç”¨ä¾‹

**éå½“å‰ç›®æ ‡**ï¼ˆåç»­ä¼˜åŒ–ï¼‰ï¼š
- â­ï¸ SIMD ä¼˜åŒ–ï¼ˆAVX2/AVX-512ï¼‰
- â­ï¸ å¤šçº¿ç¨‹å¹¶è¡Œ
- â­ï¸ å†…å­˜ä¼˜åŒ–å’Œç¼“å­˜ä¼˜åŒ–
- â­ï¸ æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1.2 ç›®æ ‡æ¨¡å‹ï¼šTiny-GPT2-ONNX
Tiny-GPT2 æ˜¯ GPT-2 çš„è½»é‡çº§ç‰ˆæœ¬ï¼Œä¸“ä¸ºèµ„æºå—é™ç¯å¢ƒè®¾è®¡ï¼š
- **å±‚æ•°**: 6 å±‚ï¼ˆæ˜¾è‘—å°‘äºæ ‡å‡† GPT-2 çš„ 12 å±‚ï¼‰
- **éšè—å±‚ç»´åº¦**: 768
- **æ³¨æ„åŠ›å¤´æ•°**: 12
- **å¤´ç»´åº¦**: 64 (768 / 12)
- **FFN ä¸­é—´ç»´åº¦**: 3072 (4 Ã— hidden_size)
- **è¯æ±‡è¡¨å¤§å°**: 50257
- **æœ€å¤§åºåˆ—é•¿åº¦**: 1024
- **æ€»å‚æ•°é‡**: ~50Mï¼ˆç›¸æ¯” GPT-2 base çš„ 117Mï¼‰

**Tiny-GPT2 çš„ä¼˜åŠ¿**ï¼š
- æ¨ç†é€Ÿåº¦å¿« 2-3 å€
- å†…å­˜å ç”¨å‡å°‘çº¦ 50%
- æ›´é€‚åˆ CPU æ¨ç†
- è´¨é‡æŸå¤±åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆå¤šæ•°ä»»åŠ¡ï¼‰

### 1.3 å…³é”®ç®—å­éœ€æ±‚ï¼ˆæŒ‰å®ç°ä¼˜å…ˆçº§ï¼‰

#### é˜¶æ®µ1ï¼šå¿…éœ€ç®—å­ï¼ˆç¡®ä¿æ¨¡å‹èƒ½è·‘ï¼‰
1. **âœ… FastGELU** - GELU æ¿€æ´»å‡½æ•°ï¼ˆâœ… åŸºç¡€å®ç°å·²å®Œæˆï¼‰
   - æ–‡ä»¶ï¼š`my_cpu/bert/fast_gelu.h`, `fast_gelu.cc`
   - çŠ¶æ€ï¼šæ ‡é‡å®ç°å®Œæˆï¼Œå« TODO-OPTIMIZE æ ‡æ³¨
   - æµ‹è¯•ï¼šå•å…ƒæµ‹è¯•å·²å®ç° (`test/my_cpu/fast_gelu_op_test.cc`)

2. **â­ï¸ LayerNormalization** - å±‚å½’ä¸€åŒ–ï¼ˆå¾…éªŒè¯æ˜¯å¦å·²æœ‰ï¼‰
   - éœ€è¦æ£€æŸ¥ `contrib_ops/cpu/` ä¸­æ˜¯å¦æœ‰å¯ç”¨å®ç°
   - å¦‚æœ‰åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™éœ€å®ç°

3. **â­ï¸ Attention** - å¤šå¤´æ³¨æ„åŠ›ï¼ˆå¾…éªŒè¯æ˜¯å¦å·²æœ‰ï¼‰
   - éœ€è¦æ£€æŸ¥ `contrib_ops/cpu/bert/` ä¸­æ˜¯å¦æœ‰å¯ç”¨å®ç°
   - å¦‚æœ‰åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™éœ€å®ç°

#### é˜¶æ®µ2ï¼šä¼˜åŒ–ç®—å­ï¼ˆåç»­æå‡æ€§èƒ½ï¼‰
1. **SkipLayerNormalization** - èåˆæ®‹å·®å’Œå±‚å½’ä¸€åŒ– â­ï¸ ä¼˜åŒ–é¡¹
2. **EmbedLayerNormalization** - èåˆåµŒå…¥å’Œå½’ä¸€åŒ– â­ï¸ ä¼˜åŒ–é¡¹
3. **BiasGelu** - èåˆ Bias å’Œ GELU â­ï¸ ä¼˜åŒ–é¡¹

**å®ç°ç­–ç•¥**ï¼š
- å…ˆæ£€æŸ¥ ONNX Runtime å·²æœ‰çš„ç®—å­å®ç°
- å¦‚æœå·²æœ‰ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå³ä½¿æ€§èƒ½ä¸æ˜¯æœ€ä¼˜ï¼‰
- åªå®ç°ç¼ºå¤±çš„å…³é”®ç®—å­
- åœ¨ä»£ç ä¸­ç”¨æ³¨é‡Šæ ‡æ³¨ä¼˜åŒ–æœºä¼š

## 2. GPT-2 ç®—å­å®ç°æ¶æ„

### 2.1 æ¨¡å‹è®¡ç®—æµç¨‹

```
è¾“å…¥ Token IDs + Position IDs
        â†“
    Embedding Layer (Word + Position)
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Transformer Block Ã— N  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ LayerNormalizationâ”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚  Multi-Head      â”‚  â”‚
    â”‚  â”‚   Attention      â”‚  â”‚
    â”‚  â”‚   (Q,K,VçŸ©é˜µ)   â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚   Softmax        â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚  Attention Out   â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚    Residual      â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ LayerNormalizationâ”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚     MatMul       â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚      GELU        â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚     MatMul       â”‚  â”‚
    â”‚  â”‚        â†“          â”‚  â”‚
    â”‚  â”‚    Residual      â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Final LayerNorm
        â†“
    LM Head (MatMul)
        â†“
    Logits
```

### 2.2 æ ¸å¿ƒç®—å­åˆ—è¡¨ï¼ˆåŸºç¡€ç‰ˆï¼‰

#### å¿…é¡»å®ç°çš„ç®—å­

1. **âœ… FastGELU** - GELU æ¿€æ´»å‡½æ•°ï¼ˆå·²å®Œæˆï¼‰
   - âœ… åŸºç¡€ç‰ˆæœ¬ï¼šä½¿ç”¨æ ‡å‡†æ•°å­¦åº“å®ç° (`std::tanh`)
   - âœ… æ–‡ä»¶ä½ç½®ï¼š`my_cpu/bert/fast_gelu.{h,cc}`
   - âœ… å•å…ƒæµ‹è¯•ï¼š`test/my_cpu/fast_gelu_op_test.cc`
   - âœ… TODO-OPTIMIZE æ ‡æ³¨ï¼šAVX2/SSE SIMD åŠ é€Ÿï¼ˆ4-8x é¢„æœŸï¼‰
   - âœ… TODO-OPTIMIZE æ ‡æ³¨ï¼šOpenMP å¹¶è¡ŒåŒ–
   - å®ç°ç‰¹ç‚¹ï¼š
     * æ”¯æŒä»»æ„å½¢çŠ¶çš„è¾“å…¥å¼ é‡
     * æ”¯æŒå¯é€‰çš„ bias è¾“å…¥ï¼ˆä¸º BiasGelu èåˆé¢„ç•™ï¼‰
     * å®Œæ•´çš„é”™è¯¯å¤„ç†
     * ç²¾åº¦ï¼š< 1e-3 è¯¯å·®

2. **â­ï¸ LayerNormalization** - å±‚å½’ä¸€åŒ–ï¼ˆå¾…éªŒè¯ï¼‰
   - æ£€æŸ¥ contrib_ops æ˜¯å¦å·²å®ç°
   - å¦‚å·²æœ‰åˆ™ç›´æ¥ä½¿ç”¨
   - ğŸ“ ä¼˜åŒ–ç‚¹ï¼šOpenMP å¹¶è¡Œ

3. **â­ï¸ Attention** - å¤šå¤´æ³¨æ„åŠ›ï¼ˆå¾…éªŒè¯ï¼‰
   - æ£€æŸ¥ contrib_ops æ˜¯å¦å·²å®ç°
   - å¦‚å·²æœ‰åˆ™ç›´æ¥ä½¿ç”¨
   - ğŸ“ ä¼˜åŒ–ç‚¹ï¼šèåˆ QKV æŠ•å½±
   - ğŸ“ ä¼˜åŒ–ç‚¹ï¼šä¼˜åŒ– Softmax

#### å¯é€‰ä¼˜åŒ–ç®—å­ï¼ˆåç»­å®ç°ï¼‰
- **SkipLayerNormalization** â­ï¸ èåˆæ®‹å·®è¿æ¥
- **EmbedLayerNormalization** â­ï¸ èåˆåµŒå…¥å±‚
- **BiasGelu** â­ï¸ èåˆ Bias å’Œ GELU

### 2.3 Tiny-GPT2 å®ç°ç­–ç•¥

```cpp
// Tiny-GPT2 æ¨¡å‹å‚æ•°
constexpr int TINY_GPT2_LAYERS = 6;
constexpr int TINY_GPT2_HIDDEN_SIZE = 768;
constexpr int TINY_GPT2_NUM_HEADS = 12;
constexpr int TINY_GPT2_HEAD_SIZE = 64;
constexpr int TINY_GPT2_FFN_SIZE = 3072;
```

**å®ç°ç­–ç•¥ï¼ˆåˆ†é˜¶æ®µï¼‰**ï¼š

#### é˜¶æ®µ 1ï¼šåŸºç¡€åŠŸèƒ½ï¼ˆ1-2å‘¨ï¼‰âœ… éƒ¨åˆ†å®Œæˆ
- âœ… **å·²å®Œæˆ** å®ç° FastGELU åŸºç¡€ç‰ˆæœ¬ï¼ˆæ ‡é‡è®¡ç®—ï¼‰
  - æ–‡ä»¶ï¼š`my_cpu/bert/fast_gelu.{h,cc}`
  - åŒ…å«å®Œæ•´çš„ TODO-OPTIMIZE æ ‡æ³¨
- âœ… **å·²å®Œæˆ** ç¼–å†™å•å…ƒæµ‹è¯•ç¡®ä¿æ­£ç¡®æ€§
  - æ–‡ä»¶ï¼š`test/my_cpu/fast_gelu_op_test.cc`
  - è¦†ç›–ï¼šåŸºç¡€åŠŸèƒ½ã€è¾¹ç•Œæƒ…å†µã€ä¸åŒå½¢çŠ¶ã€å¤§å¼ é‡
- âœ… **å·²å®Œæˆ** æ„å»ºé…ç½®å’Œæ–‡æ¡£
  - CMakeLists.txt (my_cpu + test)
  - README.md, INTEGRATION.md, QUICKSTART.md
- â­ï¸ **å¾…å®Œæˆ** éªŒè¯/ä½¿ç”¨å·²æœ‰çš„ LayerNormalization
- â­ï¸ **å¾…å®Œæˆ** éªŒè¯/ä½¿ç”¨å·²æœ‰çš„ Attention
- â­ï¸ **å¾…å®Œæˆ** é›†æˆåˆ°ä¸»æ„å»ºç³»ç»Ÿå¹¶ç¼–è¯‘
- â­ï¸ **å¾…å®Œæˆ** ç«¯åˆ°ç«¯æµ‹è¯• Tiny-GPT2 æ¨ç†

#### é˜¶æ®µ 2ï¼šä¼˜åŒ–ç‰ˆæœ¬ï¼ˆåç»­ï¼Œå¯é€‰ï¼‰â­ï¸
- ğŸ“ æ·»åŠ  SIMD ä¼˜åŒ–ï¼ˆAVX2ï¼‰
- ğŸ“ æ·»åŠ å¤šçº¿ç¨‹å¹¶è¡Œï¼ˆOpenMPï¼‰
- ğŸ“ å®ç°èåˆç®—å­ï¼ˆSkipLayerNormï¼‰
- ğŸ“ å†…å­˜å’Œç¼“å­˜ä¼˜åŒ–
- ğŸ“ æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œè°ƒä¼˜

**ä¼˜åŒ–æ ‡æ³¨è§„èŒƒ**ï¼š
```cpp
// TODO-OPTIMIZE: [ä¼˜åŒ–ç±»å‹] ä¼˜åŒ–è¯´æ˜
// ä¾‹å¦‚ï¼š
// TODO-OPTIMIZE: [SIMD] å¯ä½¿ç”¨ AVX2 å‘é‡åŒ–æ­¤å¾ªç¯ï¼Œé¢„æœŸåŠ é€Ÿ 4-8x
// TODO-OPTIMIZE: [Parallel] å¯ä½¿ç”¨ OpenMP å¹¶è¡ŒåŒ–ï¼Œé€‚åˆ batch > 1
// TODO-OPTIMIZE: [Cache] å¯è°ƒæ•´æ•°æ®å¸ƒå±€ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡
```

### 2.4 å®ç°æ–¹å¼ï¼šç‹¬ç«‹ my_cpu ç›®å½•

**âœ… å·²é‡‡ç”¨ç‹¬ç«‹ç›®å½•ç»“æ„**ï¼š
- âœ… åœ¨ `onnxruntime/my_cpu/` åˆ›å»ºç‹¬ç«‹å®ç°
- âœ… ä¸ä¿®æ”¹ç°æœ‰çš„ `contrib_ops/cpu/` ä»£ç 
- âœ… å¯ä»¥å‚è€ƒ contrib_ops çš„å®ç°æ¨¡å¼
- âœ… ä¾¿äºç‹¬ç«‹ç®¡ç†å’Œç»´æŠ¤

**âœ… my_cpu ç›®å½•å·²å®ç°**ï¼š
- âœ… ä¸ç°æœ‰ä»£ç éš”ç¦»ï¼Œä¸å½±å“åŸæœ‰åŠŸèƒ½
- âœ… ä¾¿äºå•ç‹¬ç¼–è¯‘å’Œæµ‹è¯•
- âœ… å¯ä»¥è‡ªç”±é€‰æ‹©ç¼–ç é£æ ¼å’Œä¼˜åŒ–ç­–ç•¥
- âœ… æ˜“äºç§»æ¤åˆ°å…¶ä»–é¡¹ç›®
- âœ… å­¦ä¹ å’Œå®éªŒæ›´åŠ çµæ´»

**å·²åˆ›å»ºçš„ç›®å½•ç»“æ„**ï¼š
```
my_cpu/
â”œâ”€â”€ bert/
â”‚   â”œâ”€â”€ fast_gelu.h          âœ… å·²å®ç°
â”‚   â””â”€â”€ fast_gelu.cc         âœ… å·²å®ç°
â”œâ”€â”€ my_cpu_kernels.h         âœ… å·²å®ç°
â”œâ”€â”€ my_cpu_kernels.cc        âœ… å·²å®ç°
â”œâ”€â”€ CMakeLists.txt           âœ… å·²å®ç°
â”œâ”€â”€ README.md                âœ… å·²å®ç°
â”œâ”€â”€ INTEGRATION.md           âœ… å·²å®ç°
â”œâ”€â”€ QUICKSTART.md            âœ… å·²å®ç°
â”œâ”€â”€ generate_test_data.py    âœ… å·²å®ç°
â”œâ”€â”€ verify.sh                âœ… å·²å®ç°
â””â”€â”€ verify.bat               âœ… å·²å®ç°

test/my_cpu/
â”œâ”€â”€ fast_gelu_op_test.cc     âœ… å·²å®ç°
â””â”€â”€ CMakeLists.txt           âœ… å·²å®ç°
```

## 3. è¯¦ç»†å®ç°æ­¥éª¤

### 3.0 ç›®å½•ç»“æ„è§„åˆ’ï¼ˆmy_cpu ç‹¬ç«‹å®ç°ï¼‰

```
onnxruntime/
â”œâ”€â”€ my_cpu/                              # ã€æ–°å»ºã€‘è‡ªå®šä¹‰ CPU ç®—å­æ ¹ç›®å½•
â”‚   â”œâ”€â”€ CMakeLists.txt                   # CMake æ„å»ºæ–‡ä»¶
â”‚   â”œâ”€â”€ my_cpu_kernels.h                 # ç®—å­æ³¨å†Œå¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ my_cpu_kernels.cc                # ç®—å­æ³¨å†Œå®ç°
â”‚   â””â”€â”€ bert/                            # BERT/GPT ç³»åˆ—ç®—å­
â”‚       â”œâ”€â”€ fast_gelu.h                  # FastGELU å£°æ˜
â”‚       â”œâ”€â”€ fast_gelu.cc                 # FastGELU å®ç°
â”‚       â”œâ”€â”€ skip_layer_norm.h            # SkipLayerNormï¼ˆå¯é€‰ï¼‰
â”‚       â””â”€â”€ skip_layer_norm.cc
â”‚
â”œâ”€â”€ test/
â”‚   â””â”€â”€ my_cpu/                          # ã€æ–°å»ºã€‘æµ‹è¯•ç›®å½•
â”‚       â”œâ”€â”€ CMakeLists.txt
â”‚       â”œâ”€â”€ fast_gelu_op_test.cc         # FastGELU å•å…ƒæµ‹è¯•
â”‚       â””â”€â”€ skip_layer_norm_test.cc      # SkipLayerNorm æµ‹è¯•
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ my_operators/                    # ã€å·²åˆ›å»ºã€‘æ–‡æ¡£ç›®å½•
â”‚       â””â”€â”€ operator_implementation_plan.md
â”‚
â”œâ”€â”€ python/
â”‚   â””â”€â”€ tools/
â”‚       â””â”€â”€ transformers/
â”‚           â””â”€â”€ test_tiny_gpt2_my_ops.py # ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
â”‚
â””â”€â”€ contrib_ops/cpu/                     # ç°æœ‰çš„ contrib_opsï¼ˆä»…ä¾›å‚è€ƒï¼‰
    â””â”€â”€ bert/                            # å¯å‚è€ƒçš„å®ç°ç¤ºä¾‹
        â”œâ”€â”€ attention.h
        â”œâ”€â”€ layer_norm.cc
        â””â”€â”€ ...
```

**å…³é”®è¯´æ˜**ï¼š
- âœ… `my_cpu/` ä¸ `contrib_ops/` å®Œå…¨ç‹¬ç«‹
- âœ… ä½¿ç”¨ç‹¬ç«‹çš„å‘½åç©ºé—´ `onnxruntime::my_cpu`
- âœ… ç‹¬ç«‹çš„ CMake æ„å»ºé…ç½®
- âœ… å¯å‚è€ƒ contrib_ops çš„ä»£ç é£æ ¼ï¼Œä½†ä¸ä¾èµ–å®ƒ
- âœ… ä¾¿äºåç»­ç§»æ¤æˆ–ä½œä¸ºç¤ºä¾‹é¡¹ç›®

### 3.1 ç®—å­ Schema å®šä¹‰

#### 3.1.1 FusedAttention ç®—å­ç¤ºä¾‹
```cpp
// æ–‡ä»¶è·¯å¾„: onnxruntime/my_cpu/bert/attention.h
// å‚è€ƒç°æœ‰çš„ Attention ç®—å­æ‰©å±•

ONNX_OPERATOR_SCHEMA(Attention)
    .SetDomain(kMSDomain)  // "com.microsoft"
    .SinceVersion(1)
    .SetDoc("Multi-Head Self Attention for GPT-2 with optimizations")
    .Input(0, "input", "3D input tensor with shape (batch_size, sequence_length, hidden_size)", "T")
    .Input(1, "weights", "2D weights tensor for Q,K,V projection", "T")
    .Input(2, "bias", "1D bias tensor", "T")
    .Input(3, "mask_index", "Attention mask with shape (batch_size, sequence_length) or (batch_size, past_sequence_length + sequence_length)", "M", OpSchema::Optional)
    .Input(4, "past", "Past state for key and value", "T", OpSchema::Optional)
    .Output(0, "output", "3D output tensor with shape (batch_size, sequence_length, hidden_size)", "T")
    .Output(1, "present", "Present state for key and value", "T", OpSchema::Optional)
    .Attr("num_heads", "Number of attention heads", AttributeProto::INT)
    .Attr("unidirectional", "Whether to use unidirectional (causal) mask", AttributeProto::INT, static_cast<int64_t>(0))
    .TypeConstraint("T", {"tensor(float)", "tensor(float16)"}, "Constrain input and output types to float tensors")
    .TypeConstraint("M", {"tensor(int32)"}, "Constrain mask to integer types")
    .TypeAndShapeInferenceFunction([](InferenceContext& ctx) {
        propagateElemTypeFromInputToOutput(ctx, 0, 0);
        // è¾“å‡ºå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        if (hasNInputShapes(ctx, 1)) {
            propagateShapeFromInputToOutput(ctx, 0, 0);
        }
    });
```

#### 3.1.2 FastGELU ç®—å­å®šä¹‰
```cpp
// GELU(x) = x * Î¦(x) = x * 0.5 * (1 + erf(x / sqrt(2)))
// å¿«é€Ÿè¿‘ä¼¼: GELU(x) â‰ˆ 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * x^3)))

ONNX_OPERATOR_SCHEMA(FastGelu)
    .SetDomain(kMSDomain)
    .SinceVersion(1)
    .SetDoc("Fast GELU activation with tanh approximation")
    .Input(0, "X", "Input tensor", "T")
    .Input(1, "bias", "Optional bias to add before GELU", "T", OpSchema::Optional)
    .Output(0, "Y", "Output tensor", "T")
    .TypeConstraint("T", {"tensor(float)", "tensor(float16)"}, "Constrain to float tensors")
    .TypeAndShapeInferenceFunction([](InferenceContext& ctx) {
        propagateElemTypeFromInputToOutput(ctx, 0, 0);
        propagateShapeFromInputToOutput(ctx, 0, 0);
    });
```

#### 3.1.3 SkipLayerNormalization ç®—å­
```cpp
// èåˆ Add + LayerNormalization
ONNX_OPERATOR_SCHEMA(SkipLayerNormalization)
    .SetDomain(kMSDomain)
    .SinceVersion(1)
    .SetDoc("Fused Skip (residual) connection and Layer Normalization")
    .Input(0, "input", "Input tensor", "T")
    .Input(1, "skip", "Skip/Residual tensor to add", "T")
    .Input(2, "gamma", "Scale tensor", "T")
    .Input(3, "beta", "Bias tensor", "T", OpSchema::Optional)
    .Input(4, "bias", "Bias tensor for input", "T", OpSchema::Optional)
    .Output(0, "output", "Normalized output", "T")
    .Output(1, "mean", "Mean for backward", "U", OpSchema::Optional)
    .Output(2, "inv_std_var", "Inverse std variance for backward", "U", OpSchema::Optional)
    .Output(3, "input_skip_bias_sum", "Sum of input+skip+bias", "T", OpSchema::Optional)
    .Attr("epsilon", "Small value to avoid division by zero", AttributeProto::FLOAT, 1e-5f)
    .TypeConstraint("T", {"tensor(float)", "tensor(float16)"}, "Constrain to float types")
    .TypeConstraint("U", {"tensor(float)"}, "Constrain mean and variance to float");
```

### 3.2 ç®—å­æ³¨å†Œï¼ˆmy_cpu ç›®å½•ï¼‰

**âœ… å·²å®ç°çš„æ³¨å†Œä»£ç **ï¼š

```cpp
// æ–‡ä»¶è·¯å¾„: onnxruntime/my_cpu/my_cpu_kernels.cc
// çŠ¶æ€ï¼šâœ… å·²å®ç°

namespace onnxruntime {
namespace my_cpu {

// âœ… å·²å®šä¹‰ FastGelu ç®—å­ç±»
class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMSDomain, 1, float, FastGelu);

// â­ï¸ å¾…æ·»åŠ å…¶ä»–ç®—å­
// class ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMSDomain, 1, Attention);
// class ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMSDomain, 1, SkipLayerNormalization);

Status RegisterMyCpuKernels(KernelRegistry& kernel_registry) {
  static const BuildKernelCreateInfoFn function_table[] = {
      // âœ… FastGelu å·²æ³¨å†Œ
      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(
          kCpuExecutionProvider, kMSDomain, 1, float, FastGelu)>,

      // â­ï¸ TODO: æ·»åŠ å…¶ä»–ç®—å­
      // BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(
      //     kCpuExecutionProvider, kMSDomain, 1, Attention)>,
      // BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(
      //     kCpuExecutionProvider, kMSDomain, 1, SkipLayerNormalization)>,
  };

  for (auto& function : function_table) {
    ORT_RETURN_IF_ERROR(kernel_registry.Register(function()));
  }

  return Status::OK();
}

} // namespace my_cpu
} // namespace onnxruntime
```

### 3.3 CPU Kernel å®ç° - åŸºç¡€ç‰ˆæœ¬

#### 3.3.1 FastGELU - åŸºç¡€å®ç°ï¼ˆæ­£ç¡®æ€§ä¼˜å…ˆï¼‰âœ… å·²å®Œæˆ

**å®ç°çŠ¶æ€**ï¼šâœ… å®Œæ•´å®ç°ï¼Œå«ä¼˜åŒ–æ ‡æ³¨

**æ–‡ä»¶ä½ç½®**ï¼š
- âœ… `onnxruntime/my_cpu/bert/fast_gelu.h` - å¤´æ–‡ä»¶
- âœ… `onnxruntime/my_cpu/bert/fast_gelu.cc` - å®ç°æ–‡ä»¶

**å®ç°ç‰¹ç‚¹**ï¼š
- âœ… æ ‡é‡å®ç°ä½¿ç”¨ `std::tanh`
- âœ… æ”¯æŒå¯é€‰çš„ bias è¾“å…¥ï¼ˆä¸º BiasGelu èåˆé¢„ç•™ï¼‰
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæ£€æŸ¥
- âœ… TODO-OPTIMIZE æ ‡æ³¨ï¼šAVX2 SIMDï¼ˆé¢„æœŸ 4-8x åŠ é€Ÿï¼‰
- âœ… TODO-OPTIMIZE æ ‡æ³¨ï¼šOpenMP å¹¶è¡ŒåŒ–
- âœ… æ¨¡æ¿å®ä¾‹åŒ–ï¼šfloatï¼ˆfloat16 å¾…æ·»åŠ ï¼‰

**æ ¸å¿ƒä»£ç ç‰‡æ®µ**ï¼ˆå·²å®ç°ï¼‰ï¼š

```cpp
// æ–‡ä»¶è·¯å¾„: onnxruntime/my_cpu/bert/fast_gelu.h
// çŠ¶æ€ï¼šâœ… å·²å®ç°

namespace onnxruntime {
namespace my_cpu {

template <typename T>
class FastGelu final : public OpKernel {
 public:
  FastGelu(const OpKernelInfo& info) : OpKernel(info) {}
  Status Compute(OpKernelContext* context) const override;

 private:
  void ComputeGeluScalar(const T* input, T* output, size_t count) const;
  inline T ComputeGeluValue(T x) const;

  // TODO-OPTIMIZE: [SIMD] AVX2 ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¢„æœŸåŠ é€Ÿ 4-8x
  // void ComputeGeluAVX2(const T* input, T* output, size_t count) const;
};

} // namespace my_cpu
} // namespace onnxruntime
```

```cpp
// æ–‡ä»¶è·¯å¾„: onnxruntime/my_cpu/bert/fast_gelu.cc
// çŠ¶æ€ï¼šâœ… å·²å®ç°

// GELU å…¬å¼ï¼šGELU(x) â‰ˆ 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * xÂ³)))

template <typename T>
Status FastGelu<T>::Compute(OpKernelContext* context) const {
  const Tensor* input = context->Input<Tensor>(0);
  const T* input_data = input->Data<T>();
  auto& input_shape = input->Shape();

  Tensor* output = context->Output(0, input_shape);
  T* output_data = output->MutableData<T>();

  size_t count = static_cast<size_t>(input_shape.Size());

  // æ”¯æŒå¯é€‰çš„ bias è¾“å…¥
  const Tensor* bias_tensor = context->Input<Tensor>(1);
  const T* bias_data = bias_tensor ? bias_tensor->Data<T>() : nullptr;

  // TODO-OPTIMIZE: [Parallel] OpenMP å¹¶è¡ŒåŒ–
  if (bias_data) {
    size_t bias_size = static_cast<size_t>(bias_tensor->Shape().Size());
    for (size_t i = 0; i < count; ++i) {
      T x = input_data[i] + bias_data[i % bias_size];
      output_data[i] = ComputeGeluValue(x);
    }
  } else {
    ComputeGeluScalar(input_data, output_data, count);
  }

  return Status::OK();
}

// âœ… æ ‡é‡å®ç°ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰
template <typename T>
void FastGelu<T>::ComputeGeluScalar(const T* input, T* output, size_t count) const {
  constexpr T kAlpha = static_cast<T>(0.7978845608028654);  // sqrt(2/Ï€)
  constexpr T kBeta = static_cast<T>(0.044715);
  constexpr T kHalf = static_cast<T>(0.5);

  // TODO-OPTIMIZE: [SIMD] AVX2 å¯ä¸€æ¬¡å¤„ç† 8 ä¸ª floatï¼ŒåŠ é€Ÿ 6-8x
  for (size_t i = 0; i < count; ++i) {
    T x = input[i];
    T x_cubed = x * x * x;
    T inner = kAlpha * (x + kBeta * x_cubed);
    T tanh_inner = std::tanh(inner);
    output[i] = kHalf * x * (static_cast<T>(1.0) + tanh_inner);
  }
}

// âœ… æ¨¡æ¿å®ä¾‹åŒ–
template class FastGelu<float>;
// TODO: float16 æ”¯æŒ
// template class FastGelu<MLFloat16>;

} // namespace my_cpu
} // namespace onnxruntime
```
  constexpr T kBeta = static_cast<T>(0.044715);
  constexpr T kHalf = static_cast<T>(0.5);

  T x_cubed = x * x * x;
  T inner = kAlpha * (x + kBeta * x_cubed);
  T tanh_inner = std::tanh(inner);
  return kHalf * x * (static_cast<T>(1.0) + tanh_inner);
}

// æ¨¡æ¿å®ä¾‹åŒ–
template class FastGelu<float>;
// template class FastGelu<MLFloat16>;  // TODO: åç»­æ·»åŠ  FP16 æ”¯æŒ

} // namespace my_cpu
} // namespace onnxruntime
```

**SIMD ä¼˜åŒ–ç¤ºä¾‹ï¼ˆæ ‡æ³¨åœ¨ä»£ç ä¸­ï¼Œä¸ç«‹å³å®ç°ï¼‰**ï¼š
```cpp
// TODO-OPTIMIZE: [SIMD] AVX2 ä¼˜åŒ–ç‰ˆæœ¬å‚è€ƒ
/*
#ifdef __AVX2__
#include <immintrin.h>

template <>
void FastGelu<float>::ComputeGeluAVX2(const float* input, float* output, size_t count) const {
  const size_t vec_count = count / 8;
  const size_t remainder = count % 8;

  const __m256 kAlpha = _mm256_set1_ps(0.7978845608028654f);
  const __m256 kBeta = _mm256_set1_ps(0.044715f);
  const __m256 kHalf = _mm256_set1_ps(0.5f);
  const __m256 kOne = _mm256_set1_ps(1.0f);

  for (size_t i = 0; i < vec_count; ++i) {
    __m256 x = _mm256_loadu_ps(input + i * 8);
    // ... å‘é‡åŒ–è®¡ç®— ...
    _mm256_storeu_ps(output + i * 8, result);
  }

  // å¤„ç†å‰©ä½™å…ƒç´ 
  ComputeGeluScalar(input + vec_count * 8, output + vec_count * 8, remainder);
}
#endif
*/
```

#### 3.3.2 éªŒè¯ç°æœ‰ç®—å­

```cpp
// æ–‡ä»¶è·¯å¾„: tools/check_existing_ops.cpp
// ç”¨äºæ£€æŸ¥ ONNX Runtime ä¸­å·²æœ‰å“ªäº›ç®—å­

#include <iostream>
#include <onnxruntime/core/session/onnxruntime_cxx_api.h>

void CheckExistingOperators() {
    // æ£€æŸ¥ LayerNormalization
    std::cout << "Checking for LayerNormalization..." << std::endl;
    // æŸ¥çœ‹ contrib_ops/cpu/ ç›®å½•

    // æ£€æŸ¥ Attention
    std::cout << "Checking for Attention..." << std::endl;
    // æŸ¥çœ‹ contrib_ops/cpu/bert/attention.cc

    // å¦‚æœå·²å­˜åœ¨ï¼Œè¾“å‡ºè·¯å¾„å’Œç‰ˆæœ¬ä¿¡æ¯
    // å¦‚æœä¸å­˜åœ¨ï¼Œæ ‡è®°éœ€è¦å®ç°
}

int main() {
    CheckExistingOperators();
    return 0;
}
```

**éªŒè¯æ­¥éª¤**ï¼š
```bash
# 1. æœç´¢ç°æœ‰ç®—å­å®ç°
cd onnxruntime
grep -r "LayerNormalization" contrib_ops/cpu/
grep -r "class Attention" contrib_ops/cpu/

# 2. æŸ¥çœ‹ç°æœ‰ contrib_opsï¼ˆå‚è€ƒç”¨ï¼‰
ls -la contrib_ops/cpu/bert/

# 3. æ£€æŸ¥ç®—å­æ³¨å†Œï¼ˆå‚è€ƒç”¨ï¼‰
grep -r "LayerNormalization" contrib_ops/cpu/cpu_contrib_kernels.cc

# æ³¨æ„ï¼šæˆ‘ä»¬çš„å®ç°åœ¨ my_cpu/ ç›®å½•ä¸‹ï¼Œç‹¬ç«‹äº contrib_ops/
```

### 3.4 CMake æ„å»ºé…ç½®

**âœ… å·²å®ç°çš„æ„å»ºé…ç½®**ï¼š

```cmake
# æ–‡ä»¶è·¯å¾„: onnxruntime/my_cpu/CMakeLists.txt
# çŠ¶æ€ï¼šâœ… å·²å®ç°

# âœ… å·²å®šä¹‰çš„æºæ–‡ä»¶åˆ—è¡¨
set(onnxruntime_my_cpu_srcs
  ${ONNXRUNTIME_ROOT}/my_cpu/bert/fast_gelu.cc
  ${ONNXRUNTIME_ROOT}/my_cpu/bert/fast_gelu.h
  ${ONNXRUNTIME_ROOT}/my_cpu/my_cpu_kernels.cc
  ${ONNXRUNTIME_ROOT}/my_cpu/my_cpu_kernels.h
)

# TODO: å¾…æ·»åŠ æ›´å¤šæºæ–‡ä»¶
# ${ONNXRUNTIME_ROOT}/my_cpu/bert/skip_layer_norm.cc
# ${ONNXRUNTIME_ROOT}/my_cpu/bert/skip_layer_norm.h

# âœ… åˆ›å»ºé™æ€åº“
add_library(onnxruntime_my_cpu STATIC ${onnxruntime_my_cpu_srcs})

# âœ… æ·»åŠ åŒ…å«è·¯å¾„
target_include_directories(onnxruntime_my_cpu PRIVATE
  ${ONNXRUNTIME_ROOT}
  ${ONNXRUNTIME_ROOT}/core
)

# âœ… é“¾æ¥ä¾èµ–
target_link_libraries(onnxruntime_my_cpu PUBLIC
  onnxruntime_common
  onnxruntime_framework
)

# TODO-OPTIMIZE: [SIMD] AVX2 ä¼˜åŒ–æ—¶å¯ç”¨ï¼ˆå·²é¢„ç•™ï¼‰
# if(MSVC)
#   set_source_files_properties(
#     ${ONNXRUNTIME_ROOT}/my_cpu/bert/fast_gelu.cc
#     PROPERTIES COMPILE_FLAGS "/arch:AVX2"
#   )
# elseif(CMAKE_CXX_COMPILER_ID MATCHES "GNU|Clang")
#   set_source_files_properties(
#     ${ONNXRUNTIME_ROOT}/my_cpu/bert/fast_gelu.cc
#     PROPERTIES COMPILE_FLAGS "-mavx2 -mfma"
#   )
# endif()
```

**â­ï¸ å¾…å®Œæˆï¼šé›†æˆåˆ°ä¸»æ„å»ºç³»ç»Ÿ**ï¼š
```cmake
# åœ¨ onnxruntime/CMakeLists.txt ä¸­æ·»åŠ ï¼ˆå¾…æ‰§è¡Œï¼‰
add_subdirectory(my_cpu)

# é“¾æ¥åˆ° onnxruntime ä¸»åº“ï¼ˆå¾…æ‰§è¡Œï¼‰
target_link_libraries(onnxruntime PRIVATE onnxruntime_my_cpu)
```

**è¯´æ˜**ï¼š
- âœ… æ„å»ºé…ç½®æ–‡ä»¶å·²åˆ›å»º
- âœ… åŒ…å«å®Œæ•´çš„ç¼–è¯‘é€‰é¡¹å’Œä¾èµ–
- â­ï¸ éœ€è¦ä¿®æ”¹ä¸» CMakeLists.txt ä»¥é›†æˆ
- ğŸ“š è¯¦ç»†æ­¥éª¤è§ `my_cpu/INTEGRATION.md`

## 4. æµ‹è¯•ç­–ç•¥ï¼ˆåŸºç¡€ç‰ˆï¼‰

### 4.1 å•å…ƒæµ‹è¯• - ç®—å­çº§åˆ«ï¼ˆç¡®ä¿æ­£ç¡®æ€§ï¼‰âœ… å·²å®ç°

**æµ‹è¯•æ–‡ä»¶**ï¼šâœ… `onnxruntime/test/my_cpu/fast_gelu_op_test.cc`

**æµ‹è¯•è¦†ç›–**ï¼š
- âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼ˆ`BasicFloat32`ï¼‰
- âœ… ä¸åŒå¼ é‡å½¢çŠ¶ï¼ˆ`DifferentShapes`ï¼‰
- âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•ï¼ˆ`EdgeCases`ï¼‰
- âœ… å•å…ƒç´ æµ‹è¯•ï¼ˆ`SingleElement`ï¼‰
- âœ… å¤§å¼ é‡æµ‹è¯•ï¼ˆ`LargeTensor`ï¼ŒTiny-GPT2 è§„æ¨¡ï¼‰

**æ ¸å¿ƒæµ‹è¯•ä»£ç **ï¼ˆå·²å®ç°ï¼‰ï¼š

```cpp
// æ–‡ä»¶è·¯å¾„: onnxruntime/test/my_cpu/fast_gelu_op_test.cc
// çŠ¶æ€ï¼šâœ… å·²å®ç°å®Œæ•´æµ‹è¯•å¥—ä»¶

namespace onnxruntime {
namespace test {

// âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•
TEST(FastGeluTest, BasicFloat32) {
  OpTester test("FastGelu", 1, kMSDomain);

  std::vector<int64_t> shape = {2, 3};
  std::vector<float> input = {
      -1.0f, 0.0f, 1.0f,
      -0.5f, 0.5f, 2.0f
  };

  // ä½¿ç”¨å‚è€ƒå®ç°è®¡ç®—çš„æœŸæœ›è¾“å‡º
  std::vector<float> expected_output = {
      -0.158655f, 0.0f, 0.841345f,
      -0.154269f, 0.345735f, 1.954500f
  };

  test.AddInput<float>("X", shape, input);
  test.AddOutput<float>("Y", shape, expected_output);
  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kCpuExecutionProvider});
}

// âœ… æµ‹è¯•ä¸åŒå½¢çŠ¶
TEST(FastGeluTest, DifferentShapes) {
  // æµ‹è¯• 3D å¼ é‡ [1, 4, 2]
  // ...ï¼ˆå·²å®ç°ï¼‰
}

// âœ… æµ‹è¯•è¾¹ç•Œæƒ…å†µ
TEST(FastGeluTest, EdgeCases) {
  // æµ‹è¯•å¤§è´Ÿæ•°ã€æ¥è¿‘é›¶ã€å¤§æ­£æ•°
  std::vector<float> input = {
      -10.0f,   // å¤§è´Ÿæ•°
      -0.001f,  // æ¥è¿‘é›¶è´Ÿæ•°
      0.0f,     // é›¶
      0.001f,   // æ¥è¿‘é›¶æ­£æ•°
      10.0f     // å¤§æ­£æ•°
  };
  // ...ï¼ˆå·²å®ç°ï¼‰
}

// âœ… æµ‹è¯•å¤§å¼ é‡ï¼ˆTiny-GPT2 è§„æ¨¡ï¼‰
TEST(FastGeluTest, LargeTensor) {
  // å½¢çŠ¶ï¼š[1, 8, 768] - å…¸å‹çš„ Tiny-GPT2 hidden state
  std::vector<int64_t> shape = {1, 8, 768};
  // ...ï¼ˆå·²å®ç°ï¼‰
}

// TODO-OPTIMIZE: [Test] æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå·²æ ‡æ³¨ï¼‰
/*
TEST(FastGeluTest, DISABLED_BenchmarkPerformance) {
  // æ¯”è¾ƒåŸºç¡€ç‰ˆæœ¬ vs ä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½
  // ...
}
*/

} // namespace test
} // namespace onnxruntime
```

**æµ‹è¯•å·¥å…·**ï¼š
- âœ… `my_cpu/generate_test_data.py` - Python æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
  - ä½¿ç”¨ PyTorch GELU ä½œä¸ºå‚è€ƒ
  - ç”Ÿæˆ C++ æ ¼å¼çš„æµ‹è¯•æ•°æ®
  - æ¯”è¾ƒ PyTorch vs tanh è¿‘ä¼¼çš„ç²¾åº¦
  };

  std::vector<float> expected_output;  // è®¡ç®—æœŸæœ›å€¼
  // ... å¡«å…… expected_output

  test.AddInput<float>("X", shape, input);
  test.AddOutput<float>("Y", shape, expected_output);
  test.Run();
}

// æµ‹è¯•è¾¹ç•Œæƒ…å†µ
TEST(FastGeluTest, EdgeCases) {
  OpTester test("FastGelu", 1, kMSDomain);

  std::vector<int64_t> shape = {5};
  std::vector<float> input = {
      -10.0f,   // å¤§è´Ÿæ•°
      -0.001f,  // æ¥è¿‘é›¶çš„è´Ÿæ•°
      0.0f,     // é›¶
      0.001f,   // æ¥è¿‘é›¶çš„æ­£æ•°
      10.0f     // å¤§æ­£æ•°
  };

  std::vector<float> expected_output;  // éªŒè¯è¾¹ç•Œæƒ…å†µ
  // ...

  test.AddInput<float>("X", shape, input);
  test.AddOutput<float>("Y", shape, expected_output);
  test.Run();
}

// TODO-OPTIMIZE: [Test] æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
/*
TEST(FastGeluTest, DISABLED_BenchmarkPerformance) {
  // æ¯”è¾ƒåŸºç¡€ç‰ˆæœ¬ vs ä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½
  // ...
}
*/

} // namespace test
} // namespace onnxruntime
```

**ç”Ÿæˆæµ‹è¯•æ•°æ®çš„è¾…åŠ©è„šæœ¬**ï¼š
```python
# æ–‡ä»¶è·¯å¾„: tools/generate_test_data.py
import numpy as np
import torch
import torch.nn.functional as F

def gelu_reference(x):
    """PyTorch çš„ GELU å®ç°ä½œä¸ºå‚è€ƒ"""
    return F.gelu(torch.tensor(x)).numpy()

def generate_fast_gelu_test_data():
    """ç”Ÿæˆ FastGELU æµ‹è¯•æ•°æ®"""
    test_cases = []

    # åŸºç¡€æµ‹è¯•
    input1 = np.array([[-1.0, 0.0, 1.0], [-0.5, 0.5, 2.0]], dtype=np.float32)
    output1 = gelu_reference(input1)
    test_cases.append(("BasicFloat32", input1, output1))

    # è¾¹ç•Œæµ‹è¯•
    input2 = np.array([-10.0, -0.001, 0.0, 0.001, 10.0], dtype=np.float32)
    output2 = gelu_reference(input2)
    test_cases.append(("EdgeCases", input2, output2))

    # ç”Ÿæˆ C++ ä»£ç 
    for name, inp, out in test_cases:
        print(f"// Test case: {name}")
        print(f"std::vector<float> input = {{{', '.join(f'{v}f' for v in inp.flatten())}}};")
        print(f"std::vector<float> expected = {{{', '.join(f'{v:.6f}f' for v in out.flatten())}}};")
        print()

if __name__ == "__main__":
    generate_fast_gelu_test_data()
```

### 4.2 æ¨¡å‹çº§åˆ«æµ‹è¯• - Tiny-GPT2 ç«¯åˆ°ç«¯

```python
# æ–‡ä»¶è·¯å¾„: onnxruntime/test/python/transformers/test_tiny_gpt2_custom_ops.py
import unittest
import numpy as np
import onnx
from onnx import helper, TensorProto
import onnxruntime as ort
import torch

class TestTinyGPT2CustomOps(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """åŠ è½½ Tiny-GPT2-ONNX æ¨¡å‹"""
        cls.model_path = "tiny-gpt2.onnx"  # ä½ çš„æ¨¡å‹è·¯å¾„

        # Tiny-GPT2 å‚æ•°
        cls.num_layers = 6
        cls.hidden_size = 768
        cls.num_heads = 12
        cls.vocab_size = 50257
        cls.max_seq_length = 1024

    def test_load_and_optimize_model(self):
        """æµ‹è¯•åŠ è½½å’Œä¼˜åŒ– Tiny-GPT2 æ¨¡å‹"""
        # åŠ è½½åŸå§‹æ¨¡å‹
        original_model = onnx.load(self.model_path)
        print(f"Original model nodes: {len(original_model.graph.node)}")

        # ä½¿ç”¨ ONNX Runtime ä¼˜åŒ–å·¥å…·
        from onnxruntime.transformers import optimizer
        from onnxruntime.transformers.fusion_options import FusionOptions

        opt_options = FusionOptions("gpt2")
        opt_options.enable_gelu_approximation = True
        opt_options.enable_skip_layer_norm = True
        opt_options.enable_attention = True
        opt_options.enable_bias_skip_layer_norm = True

        optimized_model = optimizer.optimize_model(
            self.model_path,
            model_type="gpt2",
            num_heads=self.num_heads,
            hidden_size=self.hidden_size,
            optimization_options=opt_options
        )
        optimized_model.save_model_to_file("tiny_gpt2_optimized.onnx")

        print(f"Optimized model nodes: {len(optimized_model.model.graph.node)}")

        # ç»Ÿè®¡ä¼˜åŒ–ç®—å­
        op_counts = {}
        for node in optimized_model.model.graph.node:
            op_type = node.op_type
            op_counts[op_type] = op_counts.get(op_type, 0) + 1

        print("\nOptimized operators:")
        for op_type, count in sorted(op_counts.items()):
            print(f"  {op_type}: {count}")

    def test_inference_single_token(self):
        """æµ‹è¯•å• token æ¨ç†ï¼ˆå¸¸è§åœºæ™¯ï¼‰"""
        # åˆ›å»ºä¼šè¯
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = 4

        session = ort.InferenceSession(
            "tiny_gpt2_optimized.onnx",
            sess_options,
            providers=["CPUExecutionProvider"]
        )

        # å• token è¾“å…¥ï¼ˆæœ€å¸¸è§çš„ç”Ÿæˆåœºæ™¯ï¼‰
        batch_size = 1
        seq_length = 1

        input_ids = np.random.randint(0, self.vocab_size, (batch_size, seq_length), dtype=np.int64)
        attention_mask = np.ones((batch_size, seq_length), dtype=np.int64)

        # æ¨ç†
        ort_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        }

        outputs = session.run(None, ort_inputs)
        logits = outputs[0]

        print(f"Output shape: {logits.shape}")
        self.assertEqual(logits.shape, (batch_size, seq_length, self.vocab_size))

    def test_text_generation_greedy(self):
        """æµ‹è¯•è´ªå¿ƒè§£ç æ–‡æœ¬ç”Ÿæˆ"""
        import time

        session = ort.InferenceSession(
            "tiny_gpt2_optimized.onnx",
            providers=["CPUExecutionProvider"]
        )

        # èµ·å§‹ promptï¼ˆä½¿ç”¨ç®€å•çš„ token IDsï¼‰
        prompt = "Hello, how are you"
        # ç®€åŒ–ï¼šå‡è®¾å·²ç»ç¼–ç ä¸º token IDs
        input_ids = np.array([[15496, 11, 703, 389, 345]], dtype=np.int64)  # ç¤ºä¾‹ IDs

        max_new_tokens = 50
        generated_ids = input_ids.copy()

        generation_times = []

        print(f"\nGenerating {max_new_tokens} tokens...")
        for step in range(max_new_tokens):
            start_time = time.perf_counter()

            # æ¨ç†
            ort_inputs = {
                "input_ids": generated_ids,
                "attention_mask": np.ones_like(generated_ids)
            }
            outputs = session.run(None, ort_inputs)
            logits = outputs[0]

            # è·å–ä¸‹ä¸€ä¸ª tokenï¼ˆè´ªå¿ƒï¼‰
            next_token_logits = logits[0, -1, :]
            next_token = np.argmax(next_token_logits)

            # è¿½åŠ åˆ°åºåˆ—
            generated_ids = np.concatenate([
                generated_ids,
                np.array([[next_token]], dtype=np.int64)
            ], axis=1)

            inference_time = (time.perf_counter() - start_time) * 1000
            generation_times.append(inference_time)

            # åœæ­¢æ¡ä»¶ï¼ˆç¤ºä¾‹ï¼šé‡åˆ° EOS token 50256ï¼‰
            if next_token == 50256:
                break

            if (step + 1) % 10 == 0:
                avg_time = np.mean(generation_times[-10:])
                print(f"  Step {step+1}: avg {avg_time:.2f}ms/token")

        # ç»Ÿè®¡
        print(f"\nGeneration complete!")
        print(f"  Total tokens: {len(generated_ids[0])}")
        print(f"  Avg latency: {np.mean(generation_times):.2f}ms/token")
        print(f"  First token: {generation_times[0]:.2f}ms")
        print(f"  Subsequent tokens: {np.mean(generation_times[1:]):.2f}ms")
        print(f"  Throughput: {1000/np.mean(generation_times):.2f} tokens/sec")

    def test_batch_inference(self):
        """æµ‹è¯•æ‰¹é‡æ¨ç†ï¼ˆå¤šä¸ªåºåˆ—ï¼‰"""
        session = ort.InferenceSession(
            "tiny_gpt2_optimized.onnx",
            providers=["CPUExecutionProvider"]
        )

        # ä¸åŒé•¿åº¦çš„åºåˆ—
        batch_size = 4
        max_seq_len = 128

        # åˆ›å»ºå˜é•¿è¾“å…¥ï¼ˆå®é™…åº”ç”¨ä¸­å¸¸è§ï¼‰
        input_ids = []
        attention_masks = []

        for i in range(batch_size):
            seq_len = np.random.randint(32, max_seq_len)
            ids = np.random.randint(0, self.vocab_size, (seq_len,), dtype=np.int64)

            # Padding åˆ°æœ€å¤§é•¿åº¦
            padded_ids = np.pad(ids, (0, max_seq_len - seq_len), constant_values=50256)
            mask = np.concatenate([np.ones(seq_len), np.zeros(max_seq_len - seq_len)])

            input_ids.append(padded_ids)
            attention_masks.append(mask)

        input_ids = np.array(input_ids, dtype=np.int64)
        attention_masks = np.array(attention_masks, dtype=np.int64)

        # æ¨ç†
        ort_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_masks
        }

        import time
        start = time.perf_counter()
        outputs = session.run(None, ort_inputs)
        elapsed = (time.perf_counter() - start) * 1000

        print(f"\nBatch inference:")
        print(f"  Batch size: {batch_size}")
        print(f"  Max seq length: {max_seq_len}")
        print(f"  Latency: {elapsed:.2f}ms")
        print(f"  Per-sample: {elapsed/batch_size:.2f}ms")

if __name__ == "__main__":
    unittest.main()
```

### 4.3 æ€§èƒ½åŸºå‡†æµ‹è¯• - Tiny-GPT2 ä¸“ç”¨

```python
# æ–‡ä»¶è·¯å¾„: onnxruntime/test/python/transformers/benchmark_tiny_gpt2.py
import time
import numpy as np
import onnxruntime as ort
import psutil
import json

class TinyGPT2Benchmark:
    def __init__(self, model_path, num_threads=4):
        """åˆå§‹åŒ– Tiny-GPT2 åŸºå‡†æµ‹è¯•"""
        self.model_path = model_path
        self.vocab_size = 50257
        self.max_seq_length = 1024

        # åˆ›å»ºä¼˜åŒ–çš„ä¼šè¯
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = num_threads
        sess_options.inter_op_num_threads = 1
        sess_options.enable_mem_pattern = True
        sess_options.enable_cpu_mem_arena = True

        self.session = ort.InferenceSession(
            model_path,
            sess_options,
            providers=["CPUExecutionProvider"]
        )

        print(f"Loaded model: {model_path}")
        print(f"Threads: {num_threads}")
        print(f"Providers: {self.session.get_providers()}")

    def benchmark_latency(self, batch_sizes=[1], seq_lengths=[1, 16, 32, 64, 128],
                         num_iterations=100, warmup=10):
        """æµ‹è¯•æ¨ç†å»¶è¿Ÿ"""
        results = []

        for batch_size in batch_sizes:
            for seq_length in seq_lengths:
                # å‡†å¤‡è¾“å…¥
                input_ids = np.random.randint(
                    0, self.vocab_size,
                    (batch_size, seq_length),
                    dtype=np.int64
                )
                attention_mask = np.ones((batch_size, seq_length), dtype=np.int64)

                ort_inputs = {
                    "input_ids": input_ids,
                    "attention_mask": attention_mask
                }

                # é¢„çƒ­
                for _ in range(warmup):
                    self.session.run(None, ort_inputs)

                # è®¡æ—¶
                latencies = []
                for _ in range(num_iterations):
                    start_time = time.perf_counter()
                    outputs = self.session.run(None, ort_inputs)
                    end_time = time.perf_counter()
                    latencies.append((end_time - start_time) * 1000)

                # ç»Ÿè®¡
                result = {
                    "batch_size": batch_size,
                    "seq_length": seq_length,
                    "mean_latency_ms": np.mean(latencies),
                    "std_latency_ms": np.std(latencies),
                    "min_latency_ms": np.min(latencies),
                    "p50_latency_ms": np.percentile(latencies, 50),
                    "p95_latency_ms": np.percentile(latencies, 95),
                    "p99_latency_ms": np.percentile(latencies, 99),
                    "throughput_samples_per_sec": 1000 * batch_size / np.mean(latencies),
                    "throughput_tokens_per_sec": 1000 * batch_size * seq_length / np.mean(latencies)
                }
                results.append(result)

                print(f"Batch={batch_size}, SeqLen={seq_length:3d}: "
                      f"{result['mean_latency_ms']:6.2f}ms Â± {result['std_latency_ms']:5.2f}ms "
                      f"(p95: {result['p95_latency_ms']:6.2f}ms), "
                      f"{result['throughput_tokens_per_sec']:7.1f} tokens/s")

        return results

    def benchmark_generation(self, num_prompts=10, max_new_tokens=50):
        """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆæ€§èƒ½ï¼ˆæœ€çœŸå®çš„åœºæ™¯ï¼‰"""
        print(f"\n=== Text Generation Benchmark ===")
        print(f"Prompts: {num_prompts}, Max new tokens: {max_new_tokens}")

        all_stats = []

        for prompt_idx in range(num_prompts):
            # éšæœºèµ·å§‹ prompt é•¿åº¦
            prompt_length = np.random.randint(5, 20)
            input_ids = np.random.randint(
                0, self.vocab_size,
                (1, prompt_length),
                dtype=np.int64
            )

            generated_ids = input_ids.copy()
            token_times = []

            # ç”Ÿæˆ tokens
            for step in range(max_new_tokens):
                ort_inputs = {
                    "input_ids": generated_ids,
                    "attention_mask": np.ones_like(generated_ids)
                }

                start_time = time.perf_counter()
                outputs = self.session.run(None, ort_inputs)
                end_time = time.perf_counter()

                token_time = (end_time - start_time) * 1000
                token_times.append(token_time)

                # è·å–ä¸‹ä¸€ä¸ª token
                logits = outputs[0]
                next_token = np.argmax(logits[0, -1, :])

                # è¿½åŠ 
                generated_ids = np.concatenate([
                    generated_ids,
                    np.array([[next_token]], dtype=np.int64)
                ], axis=1)

                # åœæ­¢æ¡ä»¶
                if next_token == 50256 or generated_ids.shape[1] >= self.max_seq_length:
                    break

            # ç»Ÿè®¡
            stats = {
                "prompt_length": prompt_length,
                "tokens_generated": len(token_times),
                "total_time_ms": sum(token_times),
                "first_token_latency_ms": token_times[0] if token_times else 0,
                "avg_token_latency_ms": np.mean(token_times) if token_times else 0,
                "tokens_per_sec": 1000 / np.mean(token_times) if token_times else 0
            }
            all_stats.append(stats)

            if (prompt_idx + 1) % 5 == 0:
                avg_first = np.mean([s["first_token_latency_ms"] for s in all_stats])
                avg_subsequent = np.mean([s["avg_token_latency_ms"] for s in all_stats])
                print(f"  Completed {prompt_idx + 1}/{num_prompts}: "
                      f"TTFT={avg_first:.2f}ms, Avg={avg_subsequent:.2f}ms/token")

        # æ€»ç»“
        print(f"\n=== Generation Summary ===")
        print(f"Time to First Token (TTFT):")
        print(f"  Mean: {np.mean([s['first_token_latency_ms'] for s in all_stats]):.2f}ms")
        print(f"  p95: {np.percentile([s['first_token_latency_ms'] for s in all_stats], 95):.2f}ms")

        print(f"Subsequent Tokens:")
        print(f"  Mean: {np.mean([s['avg_token_latency_ms'] for s in all_stats]):.2f}ms")
        print(f"  Throughput: {np.mean([s['tokens_per_sec'] for s in all_stats]):.1f} tokens/s")

        return all_stats

    def benchmark_memory(self, batch_size=1, seq_length=128):
        """æµ‹è¯•å†…å­˜å ç”¨"""
        import gc

        print(f"\n=== Memory Benchmark ===")

        # è®°å½•åˆå§‹å†…å­˜
        process = psutil.Process()
        gc.collect()
        mem_before = process.memory_info().rss / 1024 / 1024  # MB

        # æ‰§è¡Œæ¨ç†
        input_ids = np.random.randint(0, self.vocab_size, (batch_size, seq_length), dtype=np.int64)
        attention_mask = np.ones((batch_size, seq_length), dtype=np.int64)

        ort_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        }

        # å¤šæ¬¡æ¨ç†
        for _ in range(100):
            outputs = self.session.run(None, ort_inputs)

        gc.collect()
        mem_after = process.memory_info().rss / 1024 / 1024  # MB

        print(f"Memory before: {mem_before:.1f} MB")
        print(f"Memory after: {mem_after:.1f} MB")
        print(f"Memory increase: {mem_after - mem_before:.1f} MB")

        return {
            "mem_before_mb": mem_before,
            "mem_after_mb": mem_after,
            "mem_increase_mb": mem_after - mem_before
        }

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Benchmark Tiny-GPT2 on CPU")
    parser.add_argument("--model", type=str, required=True, help="Path to ONNX model")
    parser.add_argument("--threads", type=int, default=4, help="Number of threads")
    parser.add_argument("--output", type=str, default="benchmark_results.json", help="Output file")
    args = parser.parse_args()

    # åˆ›å»ºåŸºå‡†æµ‹è¯•
    benchmark = TinyGPT2Benchmark(args.model, num_threads=args.threads)

    # CPU ä¿¡æ¯
    print(f"\n=== System Info ===")
    print(f"CPU: {psutil.cpu_count(logical=False)} cores ({psutil.cpu_count(logical=True)} threads)")
    print(f"Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB")

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = {}

    # 1. å»¶è¿Ÿæµ‹è¯•ï¼ˆå…³æ³¨ batch=1ï¼‰
    print(f"\n=== Latency Benchmark ===")
    results["latency"] = benchmark.benchmark_latency(
        batch_sizes=[1, 2, 4],
        seq_lengths=[1, 8, 16, 32, 64, 128, 256],
        num_iterations=100
    )

    # 2. æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
    results["generation"] = benchmark.benchmark_generation(
        num_prompts=20,
        max_new_tokens=50
    )

    # 3. å†…å­˜æµ‹è¯•
    results["memory"] = benchmark.benchmark_memory()

    # ä¿å­˜ç»“æœ
    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nResults saved to {args.output}")

if __name__ == "__main__":
    main()
```

**è¿è¡ŒåŸºå‡†æµ‹è¯•**ï¼š
```bash
# åŸºæœ¬æµ‹è¯•
python benchmark_tiny_gpt2.py --model tiny_gpt2_optimized.onnx --threads 4

# ä¸åŒçº¿ç¨‹æ•°å¯¹æ¯”
for threads in 1 2 4 8; do
    echo "Testing with $threads threads..."
    python benchmark_tiny_gpt2.py \
        --model tiny_gpt2_optimized.onnx \
        --threads $threads \
        --output "results_${threads}threads.json"
done

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python compare_benchmark_results.py results_*.json
```

## 5. æ–‡æ¡£å’Œç¤ºä¾‹

### 5.1 ç®—å­æ–‡æ¡£æ¨¡æ¿

```markdown
# MyCustomOp

## æè¿°
è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ï¼Œå®ç°å‚æ•°åŒ–çš„çº¿æ€§ä¿®æ­£å•å…ƒã€‚

## å±æ€§
- **alpha** (float, é»˜è®¤=1.0): è´Ÿå€¼åŒºåŸŸçš„ç¼©æ”¾å› å­

## è¾“å…¥
- **X** (T): è¾“å…¥å¼ é‡ï¼Œä»»æ„å½¢çŠ¶

## è¾“å‡º
- **Y** (T): è¾“å‡ºå¼ é‡ï¼Œä¸è¾“å…¥å½¢çŠ¶ç›¸åŒ

## ç±»å‹çº¦æŸ
- **T**: tensor(float), tensor(float16)

## å…¬å¼
```
Y = X if X > 0 else alpha * X
```

## ç¤ºä¾‹
```python
import numpy as np
import onnxruntime as ort

# åˆ›å»ºåŒ…å« MyCustomOp çš„æ¨¡å‹
# ...

# è¿è¡Œæ¨ç†
x = np.array([[-2, -1, 0, 1, 2]], dtype=np.float32)
output = sess.run(None, {'X': x})
print(output)  # [[-0.2, -0.1, 0, 1, 2]]
```

## æ€§èƒ½ç‰¹æ€§
- CPU: O(n) æ—¶é—´å¤æ‚åº¦
- CUDA: é«˜åº¦å¹¶è¡ŒåŒ–ï¼Œæ”¯æŒå¤§æ‰¹é‡å¤„ç†
- å†…å­˜: åŸåœ°æ“ä½œï¼Œæ— é¢å¤–å†…å­˜å¼€é”€
```

### 5.2 ä½¿ç”¨ç¤ºä¾‹

#### 5.2.1 C++ ç¤ºä¾‹
```cpp
#include <onnxruntime/core/session/onnxruntime_cxx_api.h>

int main() {
  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "MyCustomOpExample");
  Ort::SessionOptions session_options;

  // æ³¨å†Œè‡ªå®šä¹‰ç®—å­
  OrtCustomOpDomain* domain = nullptr;
  Ort::GetApi().CreateCustomOpDomain("com.mycompany", &domain);
  // ... æ·»åŠ ç®—å­åˆ°åŸŸ
  Ort::GetApi().AddCustomOpDomain(session_options, domain);

  // åˆ›å»ºä¼šè¯
  Ort::Session session(env, "model_with_custom_op.onnx", session_options);

  // è¿è¡Œæ¨ç†
  // ...

  return 0;
}
```

#### 5.2.2 Python ç¤ºä¾‹
```python
import onnxruntime as ort
import numpy as np

# æ–¹å¼1: é€šè¿‡åŠ¨æ€åº“åŠ è½½
session_options = ort.SessionOptions()
session_options.register_custom_ops_library('path/to/custom_ops.so')

sess = ort.InferenceSession('model.onnx', session_options)

# æ–¹å¼2: ä½¿ç”¨å†…ç½®çš„ contrib ops
sess = ort.InferenceSession(
    'model.onnx',
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)

# è¿è¡Œæ¨ç†
x = np.random.randn(1, 3, 224, 224).astype(np.float32)
output = sess.run(None, {'input': x})
```

## 6. æ„å»ºå’Œéƒ¨ç½²

### 6.1 ç¼–è¯‘é€‰é¡¹ - CPU ä¼˜åŒ–

```bash
# Linux/macOS ç¼–è¯‘ - å¯ç”¨æ‰€æœ‰ CPU ä¼˜åŒ–
./build.sh \
  --config Release \
  --build_shared_lib \
  --parallel \
  --enable_pybind \
  --use_openmp \
  --cmake_extra_defines \
    CMAKE_CXX_FLAGS="-march=native -mavx2 -mfma -fopenmp" \
    onnxruntime_ENABLE_CPU_FP16=ON

# Windows ç¼–è¯‘
.\build.bat \
  --config Release \
  --build_shared_lib \
  --parallel \
  --enable_pybind \
  --use_openmp \
  --cmake_extra_defines \
    CMAKE_CXX_FLAGS="/arch:AVX2 /openmp"

# é’ˆå¯¹ç‰¹å®š CPU æ¶æ„ä¼˜åŒ–ï¼ˆä¾‹å¦‚ Intel Skylakeï¼‰
./build.sh \
  --config Release \
  --build_shared_lib \
  --parallel \
  --cmake_extra_defines \
    CMAKE_CXX_FLAGS="-march=skylake -mtune=skylake"

# å¯ç”¨ MLASï¼ˆMicrosoft Linear Algebra Subprogramsï¼‰ä¼˜åŒ–
./build.sh \
  --config Release \
  --build_shared_lib \
  --use_mlas \
  --parallel
```

### 6.2 éªŒè¯æ„å»º

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•ï¼ˆmy_cpu ç®—å­ï¼‰
cd build/Release
./onnxruntime_test_all --gtest_filter="*FastGelu*:*SkipLayerNorm*"

# è¿è¡Œ Tiny-GPT2 é›†æˆæµ‹è¯•
python onnxruntime/test/python/transformers/test_tiny_gpt2_custom_ops.py

# TODO-OPTIMIZE: [Test] æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆåç»­å®ç°ï¼‰
# python onnxruntime/test/python/transformers/benchmark_tiny_gpt2_custom_ops.py
```

### 6.3 æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²æµç¨‹

```python
# æ–‡ä»¶è·¯å¾„: scripts/optimize_and_deploy_gpt2.py
"""
GPT-2 æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²è„šæœ¬
"""
import onnx
import onnxruntime as ort
from onnxruntime.transformers import optimizer
from onnxruntime.transformers.fusion_options import FusionOptions
import argparse

def optimize_gpt2_model(input_model_path, output_model_path, opt_level=99):
    """ä¼˜åŒ– GPT-2 æ¨¡å‹"""

    # è®¾ç½®èåˆé€‰é¡¹
    fusion_options = FusionOptions("gpt2")
    fusion_options.enable_gelu_approximation = True  # ä½¿ç”¨ FastGelu
    fusion_options.enable_skip_layer_norm = True     # ä½¿ç”¨ SkipLayerNormalization
    fusion_options.enable_attention = True           # ä½¿ç”¨èåˆ Attention
    fusion_options.enable_bias_skip_layer_norm = True
    fusion_options.enable_embed_layer_norm = True

    # åˆ›å»ºä¼˜åŒ–å™¨
    model_optimizer = optimizer.optimize_model(
        input_model_path,
        model_type="gpt2",
        num_heads=12,        # GPT-2 base
        hidden_size=768,     # GPT-2 base
        opt_level=opt_level,
        optimization_options=fusion_options,
        use_gpu=False
    )

    # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
    model_optimizer.save_model_to_file(output_model_path)

    # æ‰“å°ä¼˜åŒ–ç»Ÿè®¡
    print(f"\n=== Optimization Statistics ===")
    print(f"Original nodes: {len(onnx.load(input_model_path).graph.node)}")
    print(f"Optimized nodes: {len(model_optimizer.model.graph.node)}")

    # ç»Ÿè®¡èåˆç®—å­æ•°é‡
    fused_op_counts = {}
    for node in model_optimizer.model.graph.node:
        if node.op_type not in fused_op_counts:
            fused_op_counts[node.op_type] = 0
        fused_op_counts[node.op_type] += 1

    print(f"\n=== Custom Op Usage ===")
    for op_type in ["Attention", "FastGelu", "SkipLayerNormalization", "EmbedLayerNormalization"]:
        if op_type in fused_op_counts:
            print(f"{op_type}: {fused_op_counts[op_type]}")

    return model_optimizer

def validate_optimized_model(original_path, optimized_path, test_input):
    """éªŒè¯ä¼˜åŒ–åçš„æ¨¡å‹ç²¾åº¦"""
    import numpy as np

    # åŠ è½½åŸå§‹æ¨¡å‹
    sess_orig = ort.InferenceSession(original_path, providers=["CPUExecutionProvider"])

    # åŠ è½½ä¼˜åŒ–æ¨¡å‹
    sess_opt = ort.InferenceSession(optimized_path, providers=["CPUExecutionProvider"])

    # è¿è¡Œæ¨ç†
    orig_output = sess_orig.run(None, test_input)
    opt_output = sess_opt.run(None, test_input)

    # æ¯”è¾ƒè¾“å‡º
    for i, (orig, opt) in enumerate(zip(orig_output, opt_output)):
        max_diff = np.max(np.abs(orig - opt))
        mean_diff = np.mean(np.abs(orig - opt))
        print(f"\nOutput {i}:")
        print(f"  Max diff: {max_diff:.6f}")
        print(f"  Mean diff: {mean_diff:.6f}")
        print(f"  Relative error: {mean_diff / (np.mean(np.abs(orig)) + 1e-6):.6f}")

def main():
    parser = argparse.ArgumentParser(description="Optimize GPT-2 model for CPU deployment")
    parser.add_argument("--input", type=str, required=True, help="Input ONNX model path")
    parser.add_argument("--output", type=str, required=True, help="Output optimized model path")
    parser.add_argument("--opt_level", type=int, default=99, help="Optimization level (0-99)")
    args = parser.parse_args()

    # ä¼˜åŒ–æ¨¡å‹
    optimized_model = optimize_gpt2_model(args.input, args.output, args.opt_level)

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    import numpy as np
    test_input = {
        "input_ids": np.random.randint(0, 50257, (1, 128), dtype=np.int64),
        "attention_mask": np.ones((1, 128), dtype=np.int64)
    }

    # éªŒè¯ç²¾åº¦
    print("\n=== Validating Optimized Model ===")
    validate_optimized_model(args.input, args.output, test_input)

if __name__ == "__main__":
    main()
```

### 6.4 éƒ¨ç½²é…ç½®

```python
# æ–‡ä»¶è·¯å¾„: deployment/gpt2_inference_config.py
"""
ç”Ÿäº§ç¯å¢ƒæ¨ç†é…ç½®
"""
import onnxruntime as ort
import psutil

def create_optimized_session(model_path, num_threads=None):
    """åˆ›å»ºä¼˜åŒ–çš„æ¨ç†ä¼šè¯"""

    # ä¼šè¯é€‰é¡¹
    sess_options = ort.SessionOptions()

    # è®¾ç½®çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•°ï¼‰
    if num_threads is None:
        num_threads = psutil.cpu_count(logical=False)  # ç‰©ç†æ ¸å¿ƒæ•°
    sess_options.intra_op_num_threads = num_threads
    sess_options.inter_op_num_threads = 1  # GPT-2 æ˜¯é¡ºåºæ‰§è¡Œ

    # å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    # å¯ç”¨å†…å­˜æ¨¡å¼ä¼˜åŒ–
    sess_options.enable_mem_pattern = True
    sess_options.enable_cpu_mem_arena = True

    # è®¾ç½®æ‰§è¡Œæ¨¡å¼
    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL

    # æ—¥å¿—é…ç½®
    sess_options.log_severity_level = 3  # Error only

    # åˆ›å»ºä¼šè¯
    session = ort.InferenceSession(
        model_path,
        sess_options,
        providers=["CPUExecutionProvider"]
    )

    return session

def get_cpu_info():
    """è·å– CPU ä¿¡æ¯ç”¨äºæ€§èƒ½è°ƒä¼˜"""
    import cpuinfo

    info = cpuinfo.get_cpu_info()
    print(f"CPU: {info['brand_raw']}")
    print(f"Physical cores: {psutil.cpu_count(logical=False)}")
    print(f"Logical cores: {psutil.cpu_count(logical=True)}")
    print(f"Max frequency: {psutil.cpu_freq().max:.2f} MHz")

    # æ£€æŸ¥ CPU ç‰¹æ€§
    flags = info.get('flags', [])
    simd_support = {
        'AVX': 'avx' in flags,
        'AVX2': 'avx2' in flags,
        'AVX512': any('avx512' in f for f in flags),
        'FMA': 'fma' in flags
    }

    print("\nSIMD Support:")
    for feature, supported in simd_support.items():
        print(f"  {feature}: {'âœ“' if supported else 'âœ—'}")

    return simd_support

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    get_cpu_info()

    # åˆ›å»ºä¼šè¯
    session = create_optimized_session("gpt2_optimized.onnx", num_threads=4)

    print(f"\nSession created successfully!")
    print(f"Providers: {session.get_providers()}")
```

## 7. CPU ä¼˜åŒ–æœ€ä½³å®è·µ

### 7.1 SIMD ä¼˜åŒ–æŠ€å·§

```cpp
// ç¤ºä¾‹ï¼šä½¿ç”¨ AVX2 ä¼˜åŒ–çš„å‘é‡æ“ä½œ
#ifdef __AVX2__
#include <immintrin.h>

void OptimizedVectorAdd(const float* a, const float* b, float* c, size_t n) {
    const size_t vec_size = 8;  // AVX2 å¤„ç† 8 ä¸ª float
    const size_t vec_count = n / vec_size;
    const size_t remainder = n % vec_size;

    // å‘é‡åŒ–ä¸»å¾ªç¯
    for (size_t i = 0; i < vec_count; ++i) {
        __m256 va = _mm256_loadu_ps(a + i * vec_size);
        __m256 vb = _mm256_loadu_ps(b + i * vec_size);
        __m256 vc = _mm256_add_ps(va, vb);
        _mm256_storeu_ps(c + i * vec_size, vc);
    }

    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (size_t i = vec_count * vec_size; i < n; ++i) {
        c[i] = a[i] + b[i];
    }
}
#endif
```

### 7.2 ç¼“å­˜ä¼˜åŒ– - åˆ†å—ï¼ˆTilingï¼‰

```cpp
// çŸ©é˜µä¹˜æ³•çš„åˆ†å—ä¼˜åŒ–
void TiledMatMul(
    const float* A,  // M x K
    const float* B,  // K x N
    float* C,        // M x N
    int M, int N, int K) {

    constexpr int TILE_SIZE = 64;  // æ ¹æ® L1 ç¼“å­˜å¤§å°è°ƒæ•´

    for (int i = 0; i < M; i += TILE_SIZE) {
        for (int j = 0; j < N; j += TILE_SIZE) {
            for (int k = 0; k < K; k += TILE_SIZE) {
                // åœ¨å°å—ä¸Šæ‰§è¡ŒçŸ©é˜µä¹˜æ³•
                int i_max = std::min(i + TILE_SIZE, M);
                int j_max = std::min(j + TILE_SIZE, N);
                int k_max = std::min(k + TILE_SIZE, K);

                for (int ii = i; ii < i_max; ++ii) {
                    for (int jj = j; jj < j_max; ++jj) {
                        float sum = C[ii * N + jj];
                        for (int kk = k; kk < k_max; ++kk) {
                            sum += A[ii * K + kk] * B[kk * N + jj];
                        }
                        C[ii * N + jj] = sum;
                    }
                }
            }
        }
    }
}
```

### 7.3 OpenMP å¹¶è¡ŒåŒ–

```cpp
#include <omp.h>

// ä½¿ç”¨ OpenMP å¹¶è¡ŒåŒ–æ‰¹é‡å¤„ç†
void ParallelLayerNorm(
    const float* input,
    float* output,
    const float* gamma,
    const float* beta,
    int batch_size,
    int seq_len,
    int hidden_size,
    float epsilon) {

    #pragma omp parallel for collapse(2)
    for (int b = 0; b < batch_size; ++b) {
        for (int s = 0; s < seq_len; ++s) {
            int offset = (b * seq_len + s) * hidden_size;
            const float* inp = input + offset;
            float* out = output + offset;

            // è®¡ç®—å‡å€¼
            float sum = 0.0f;
            for (int h = 0; h < hidden_size; ++h) {
                sum += inp[h];
            }
            float mean = sum / hidden_size;

            // è®¡ç®—æ–¹å·®
            float var_sum = 0.0f;
            for (int h = 0; h < hidden_size; ++h) {
                float diff = inp[h] - mean;
                var_sum += diff * diff;
            }
            float variance = var_sum / hidden_size;
            float inv_std = 1.0f / std::sqrt(variance + epsilon);

            // å½’ä¸€åŒ–
            for (int h = 0; h < hidden_size; ++h) {
                out[h] = (inp[h] - mean) * inv_std * gamma[h] + beta[h];
            }
        }
    }
}
```

### 7.4 å†…å­˜å¯¹é½å’Œé¢„å–

```cpp
// å†…å­˜å¯¹é½åˆ†é…
#include <cstdlib>

template<typename T>
T* AlignedAlloc(size_t count, size_t alignment = 64) {
    void* ptr = nullptr;
    #ifdef _WIN32
    ptr = _aligned_malloc(count * sizeof(T), alignment);
    #else
    posix_memalign(&ptr, alignment, count * sizeof(T));
    #endif
    return static_cast<T*>(ptr);
}

// ä½¿ç”¨é¢„å–æé«˜æ€§èƒ½
void PrefetchedSum(const float* data, size_t n, float& result) {
    constexpr size_t PREFETCH_DISTANCE = 64;

    result = 0.0f;
    for (size_t i = 0; i < n; ++i) {
        // é¢„å–æœªæ¥çš„æ•°æ®
        if (i + PREFETCH_DISTANCE < n) {
            __builtin_prefetch(&data[i + PREFETCH_DISTANCE], 0, 1);
        }
        result += data[i];
    }
}
```

### 7.5 æ•°å€¼ç¨³å®šæ€§

```cpp
// Softmax çš„æ•°å€¼ç¨³å®šå®ç°
void StableSoftmax(const float* input, float* output, int size) {
    // æ‰¾åˆ°æœ€å¤§å€¼é¿å…æº¢å‡º
    float max_val = input[0];
    for (int i = 1; i < size; ++i) {
        max_val = std::max(max_val, input[i]);
    }

    // è®¡ç®— exp(x - max) å’Œæ€»å’Œ
    float sum = 0.0f;
    for (int i = 0; i < size; ++i) {
        output[i] = std::exp(input[i] - max_val);
        sum += output[i];
    }

    // å½’ä¸€åŒ–
    float inv_sum = 1.0f / sum;
    for (int i = 0; i < size; ++i) {
        output[i] *= inv_sum;
    }
}

// GELU çš„æ•°å€¼ç¨³å®šè¿‘ä¼¼
inline float FastGeluApprox(float x) {
    // ä½¿ç”¨ tanh è¿‘ä¼¼ï¼Œé¿å… erf çš„æ•°å€¼é—®é¢˜
    constexpr float kAlpha = 0.7978845608f;  // sqrt(2/pi)
    constexpr float kBeta = 0.044715f;
    constexpr float kHalf = 0.5f;

    float x_cubed = x * x * x;
    float inner = kAlpha * (x + kBeta * x_cubed);

    // ä½¿ç”¨å¿«é€Ÿ tanh è¿‘ä¼¼
    float tanh_val;
    if (inner >= 0) {
        float exp_2x = std::exp(-2.0f * inner);
        tanh_val = (1.0f - exp_2x) / (1.0f + exp_2x);
    } else {
        float exp_2x = std::exp(2.0f * inner);
        tanh_val = (exp_2x - 1.0f) / (exp_2x + 1.0f);
    }

    return kHalf * x * (1.0f + tanh_val);
}
```

### 7.6 ä½¿ç”¨ MLAS åº“

```cpp
// åˆ©ç”¨ ONNX Runtime å†…ç½®çš„ MLAS ä¼˜åŒ–åº“
#include "core/mlas/inc/mlas.h"

void OptimizedMatMul(
    const float* A,
    const float* B,
    float* C,
    size_t M, size_t N, size_t K,
    concurrency::ThreadPool* thread_pool) {

    // ä½¿ç”¨ MLAS é«˜æ€§èƒ½ GEMM
    MlasGemm(
        CblasNoTrans,     // TransA
        CblasNoTrans,     // TransB
        M,                // M
        N,                // N
        K,                // K
        1.0f,             // alpha
        A,                // A
        K,                // lda
        B,                // B
        N,                // ldb
        0.0f,             // beta
        C,                // C
        N,                // ldc
        thread_pool       // thread pool
    );
}
```

### 7.7 æ€§èƒ½åˆ†æå·¥å…·

```bash
# Linux - ä½¿ç”¨ perf åˆ†æ
perf record -g ./onnxruntime_perf_test --model gpt2_optimized.onnx
perf report

# æŸ¥çœ‹çƒ­ç‚¹å‡½æ•°
perf stat -e cache-references,cache-misses,cycles,instructions \
    ./onnxruntime_perf_test --model gpt2_optimized.onnx

# Intel VTune åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
vtune -collect hotspots -r result_dir \
    ./onnxruntime_perf_test --model gpt2_optimized.onnx

# ä½¿ç”¨ gprof
g++ -pg -O2 your_code.cpp
./a.out
gprof ./a.out gmon.out > analysis.txt
```

## 8. æ•…éšœæ’æŸ¥

### 8.1 å¸¸è§é—®é¢˜

#### é—®é¢˜1: ç®—å­æœªæ‰¾åˆ°
```
Error: Cannot find kernel definition for op MyCustomOp
```
**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ç®—å­åŸŸåå’Œç‰ˆæœ¬æ˜¯å¦åŒ¹é…
- ç¡®è®¤ç®—å­å·²æ­£ç¡®æ³¨å†Œåˆ° KernelRegistry
- éªŒè¯æ‰§è¡Œæä¾›è€…ç±»å‹æ˜¯å¦æ­£ç¡®

#### é—®é¢˜2: ç±»å‹ä¸åŒ¹é…
```
Error: Type inference failed for node MyCustomOp
```
**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ TypeConstraint å®šä¹‰
- å®ç°æ­£ç¡®çš„ TypeAndShapeInferenceFunction
- éªŒè¯è¾“å…¥è¾“å‡ºç±»å‹å£°æ˜

#### é—®é¢˜3: å½¢çŠ¶æ¨æ–­é”™è¯¯
```
Error: Shape inference error for op MyCustomOp
```
**è§£å†³æ–¹æ¡ˆ**:
- å®ç°æˆ–ä¿®å¤ ShapeInferenceFunction
- æ£€æŸ¥è¾“å…¥å½¢çŠ¶çš„ä¼ æ’­é€»è¾‘
- æ·»åŠ æ›´å¤šçš„å½¢çŠ¶æ£€æŸ¥

#### é—®é¢˜4: CUDA å†…æ ¸é”™è¯¯
```
Error: CUDA error: invalid configuration argument
```
**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ grid/block é…ç½®
- éªŒè¯å…±äº«å†…å­˜ä½¿ç”¨
- ä½¿ç”¨ cuda-memcheck æ£€æŸ¥å†…å­˜è®¿é—®

### 8.2 æ€§èƒ½é—®é¢˜è¯Šæ–­

```bash
# ä½¿ç”¨ ONNX Runtime profiler
export ORT_PROFILER_ENABLED=1
export ORT_PROFILER_OUTPUT_DIR=./profiling

# NVIDIA æ€§èƒ½åˆ†æ
nsys profile -o my_custom_op ./my_app
nsys-ui my_custom_op.qdrep

# ä½¿ç”¨ perf åˆ†æ CPU æ€§èƒ½
perf record -g ./my_app
perf report
```

## 9. æŒç»­é›†æˆå’Œæµ‹è¯•

### 9.1 CI é…ç½®ç¤ºä¾‹

```yaml
# .github/workflows/custom_ops_ci.yml
name: Custom Ops CI

on: [push, pull_request]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Setup dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake ninja-build

      - name: Build
        run: |
          mkdir build && cd build
          cmake .. -GNinja
          ninja

      - name: Run tests
        run: |
          cd build
          ctest --output-on-failure

      - name: Run benchmarks
        run: |
          cd build
          ./my_custom_op_benchmark --benchmark_format=json
```

### 9.2 æµ‹è¯•è¦†ç›–ç‡

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
cmake -DCMAKE_BUILD_TYPE=Debug -DCOVERAGE=ON ..
make
make test
lcov --capture --directory . --output-file coverage.info
genhtml coverage.info --output-directory coverage_report
```

## 10. å‚è€ƒèµ„æº

### 10.1 å®˜æ–¹æ–‡æ¡£
- [ONNX Runtime è‡ªå®šä¹‰ç®—å­æ–‡æ¡£](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html)
- [ONNX ç®—å­è§„èŒƒ](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
- [ONNX Runtime C API](https://onnxruntime.ai/docs/api/c/)

### 10.2 ä»£ç ç¤ºä¾‹
- `onnxruntime/test/testdata/custom_op_library/`
- `onnxruntime/contrib_ops/`
- [ONNX Runtime Extensions](https://github.com/microsoft/onnxruntime-extensions)

### 10.3 ç›¸å…³æ–‡æ¡£
- [ContribOperators.md](../ContribOperators.md)
- [OperatorKernels.md](../OperatorKernels.md)
- [Coding_Conventions_and_Standards.md](../Coding_Conventions_and_Standards.md)

## 11. Tiny-GPT2 CPU åŸºç¡€ç‰ˆå®ç°æ—¶é—´è¡¨ï¼ˆçº¦2å‘¨ï¼‰

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ | å…³é”®äº¤ä»˜ç‰© |
|------|------|----------|-----------|
| **ç¬¬1å‘¨** | ç¯å¢ƒæ­å»ºå’ŒåŸºç¡€å®ç° | | |
| Day 1 | æ­å»ºå¼€å‘ç¯å¢ƒï¼Œç¼–è¯‘ ONNX Runtime | 1å¤© | å¯ç¼–è¯‘çš„ ORT æºç  |
| Day 2 | åˆ†æ Tiny-GPT2-ONNX æ¨¡å‹ç»“æ„ | 1å¤© | æ¨¡å‹åˆ†ææŠ¥å‘Š |
| Day 3-4 | å®ç° FastGELU åŸºç¡€ç‰ˆæœ¬ | 2å¤© | å¯å·¥ä½œçš„ FastGELU |
| Day 5 | å•å…ƒæµ‹è¯•å’Œç²¾åº¦éªŒè¯ | 1å¤© | é€šè¿‡çš„æµ‹è¯•ç”¨ä¾‹ |
| **ç¬¬2å‘¨** | é›†æˆæµ‹è¯•å’Œæ”¶å°¾ | | |
| Day 1-2 | æ£€æŸ¥/å®ç° LayerNormalization | 2å¤© | æ­£ç¡®çš„å½’ä¸€åŒ–ç»“æœ |
| Day 3 | Tiny-GPT2 ç«¯åˆ°ç«¯é›†æˆ | 1å¤© | ä¼˜åŒ–åçš„ ONNX æ¨¡å‹ |
| Day 4 | ç²¾åº¦éªŒè¯ï¼ˆå¯¹æ¯” PyTorchï¼‰ | 1å¤© | ç²¾åº¦æŠ¥å‘Šï¼ˆ< 1e-3ï¼‰ |
| Day 5 | æ–‡æ¡£å’Œä»£ç æ•´ç† | 1å¤© | å®Œæ•´æ–‡æ¡£ |

### å…³é”®é‡Œç¨‹ç¢‘ï¼ˆåŸºç¡€ç‰ˆï¼‰
- âœ… **é‡Œç¨‹ç¢‘1**ï¼ˆç¬¬1å‘¨æœ«ï¼‰: FastGELU åŸºç¡€ç‰ˆå®Œæˆï¼Œç²¾åº¦æ­£ç¡®
- âœ… **é‡Œç¨‹ç¢‘2**ï¼ˆç¬¬2å‘¨ä¸­ï¼‰: æ‰€æœ‰å¿…éœ€ç®—å­å°±ç»ª
- âœ… **é‡Œç¨‹ç¢‘3**ï¼ˆç¬¬2å‘¨æœ«ï¼‰: Tiny-GPT2 æ­£ç¡®è¿è¡Œï¼Œç²¾åº¦éªŒè¯é€šè¿‡

### åŸºç¡€ç‰ˆç›®æ ‡ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰

| æŒ‡æ ‡ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|
| **ç²¾åº¦è¯¯å·®** | **< 1e-3** | **æœ€é‡è¦ï¼šç¡®ä¿æ­£ç¡®æ€§** |
| é¦– token å»¶è¿Ÿ (TTFT) | < 100ms | åŸºç¡€ç‰ˆæœ¬ï¼Œæœªä¼˜åŒ– |
| åç»­ token å»¶è¿Ÿ | < 80ms | åŸºç¡€ç‰ˆæœ¬ï¼Œæœªä¼˜åŒ– |
| æ•´ä½“åå | > 10 tokens/s | åŸºç¡€ç‰ˆæœ¬ï¼Œæœªä¼˜åŒ– |
| å†…å­˜å ç”¨ | < 1GB | åŸºç¡€ç‰ˆæœ¬ |

### TODO-OPTIMIZE æ ‡æ³¨çš„ä¼˜åŒ–æœºä¼š

å½“åŸºç¡€ç‰ˆæœ¬è¿è¡Œæ­£ç¡®åï¼Œå¯æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å®ç°ï¼š

1. **SIMD ä¼˜åŒ–** (é¢„æœŸåŠ é€Ÿ 4-8x)
   - FastGELU AVX2 å‘é‡åŒ–
   - LayerNorm å‘é‡åŒ–

2. **å¹¶è¡Œä¼˜åŒ–** (é¢„æœŸåŠ é€Ÿ 2-4x)
   - OpenMP æ‰¹é‡å¹¶è¡Œ
   - å¤šçº¿ç¨‹ Attention

3. **ç¼“å­˜ä¼˜åŒ–** (é¢„æœŸåŠ é€Ÿ 1.5-2x)
   - çŸ©é˜µä¹˜æ³•åˆ†å—
   - å†…å­˜å¯¹é½å’Œé¢„å–

4. **æ•°å€¼ä¼˜åŒ–** (ç²¾åº¦æå‡)
   - FP16/BF16 æ··åˆç²¾åº¦
   - æ•°å€¼ç¨³å®šæ€§æ”¹è¿›

## 12. å‚è€ƒèµ„æº

### 12.1 ONNX Runtime å®˜æ–¹æ–‡æ¡£
- [ONNX Runtime è‡ªå®šä¹‰ç®—å­æ–‡æ¡£](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html)
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](https://onnxruntime.ai/docs/performance/tune-performance.html)
- [Transformer ä¼˜åŒ–](https://onnxruntime.ai/docs/performance/transformers-optimization.html)
- [ONNX Runtime C API](https://onnxruntime.ai/docs/api/c/)

### 12.2 ä»£ç å‚è€ƒï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼‰
- **ç°æœ‰å®ç°å‚è€ƒ**ï¼ˆä½äº contrib_opsï¼Œä»…ä¾›å‚è€ƒï¼Œä¸ç›´æ¥ä½¿ç”¨ï¼‰:
  - `onnxruntime/contrib_ops/cpu/bert/` - BERT ç›¸å…³ç®—å­
  - `onnxruntime/contrib_ops/cpu/bert/attention.h` - Attention å®ç°
  - `onnxruntime/contrib_ops/cpu/bert/skip_layer_norm.cc` - SkipLayerNorm
  - `onnxruntime/contrib_ops/cpu/activations.cc` - æ¿€æ´»å‡½æ•°

- **æˆ‘ä»¬çš„å®ç°ä½ç½®**ï¼ˆç‹¬ç«‹ç›®å½•ï¼‰:
  - `onnxruntime/my_cpu/bert/fast_gelu.cc` - FastGELU å®ç°
  - `onnxruntime/my_cpu/my_cpu_kernels.cc` - ç®—å­æ³¨å†Œ
  - `onnxruntime/test/my_cpu/fast_gelu_op_test.cc` - å•å…ƒæµ‹è¯•

- **æµ‹è¯•å‚è€ƒ**:
  - `onnxruntime/test/python/transformers/test_gpt2_*` - GPT-2 æµ‹è¯•ç¤ºä¾‹
  - `onnxruntime/test/python/transformers/gpt2_model_generator.py` - æ¨¡å‹ç”Ÿæˆå™¨

- **ä¼˜åŒ–å·¥å…·**ï¼ˆå¯ä¾›å‚è€ƒï¼‰:
  - `onnxruntime/python/tools/transformers/optimizer.py` - æ¨¡å‹ä¼˜åŒ–å™¨
  - `onnxruntime/python/tools/transformers/fusion_gpt_attention.py` - GPT Attention èåˆ

### 12.3 ç›¸å…³æŠ€æœ¯æ–‡æ¡£
- [ONNX ç®—å­è§„èŒƒ](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
- [GPT-2 è®ºæ–‡](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Transformer åŸç†](https://arxiv.org/abs/1706.03762)
- [GELU æ¿€æ´»å‡½æ•°](https://arxiv.org/abs/1606.08415)
- [Layer Normalization](https://arxiv.org/abs/1607.06450)

### 12.4 æ€§èƒ½ä¼˜åŒ–èµ„æº
- [Intel ä¼˜åŒ–æŒ‡å—](https://www.intel.com/content/www/us/en/developer/articles/guide/deep-learning-performance-guide.html)
- [AVX2 ç¼–ç¨‹æŒ‡å—](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/)
- [OpenMP æ•™ç¨‹](https://www.openmp.org/resources/tutorials-articles/)
- [CPU ç¼“å­˜ä¼˜åŒ–](https://en.algorithmica.org/hpc/cpu-cache/)

### 12.5 å¼€æºé¡¹ç›®å‚è€ƒ
- [ONNX Runtime Extensions](https://github.com/microsoft/onnxruntime-extensions) - è‡ªå®šä¹‰ç®—å­ç¤ºä¾‹
- [Hugging Face Transformers](https://github.com/huggingface/transformers) - GPT-2 å®ç°
- [PyTorch](https://github.com/pytorch/pytorch) - ç®—å­å®ç°å‚è€ƒ
- [MLAS](https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/core/mlas) - çº¿æ€§ä»£æ•°åº“

### 12.6 å·¥å…·å’Œåº“
- **æ€§èƒ½åˆ†æ**:
  - `perf` - Linux æ€§èƒ½åˆ†æå·¥å…·
  - Intel VTune Profiler
  - Google Benchmark
  - `gprof` - GNU æ€§èƒ½åˆ†æå™¨

- **æ•°å­¦åº“**:
  - Intel MKL (Math Kernel Library)
  - OpenBLAS
  - Eigen
  - MLAS (å†…ç½®äº ONNX Runtime)

- **SIMD åº“**:
  - Intel Intrinsics Guide
  - SLEEF (SIMD Library for Evaluating Elementary Functions)

### 12.7 ONNX Runtime å†…éƒ¨æ–‡æ¡£
- [ContribOperators.md](../ContribOperators.md) - Contrib ç®—å­è¯´æ˜
- [OperatorKernels.md](../OperatorKernels.md) - Kernel å®ç°æŒ‡å—
- [Coding_Conventions_and_Standards.md](../Coding_Conventions_and_Standards.md) - ç¼–ç è§„èŒƒ
- [cmake_guideline.md](../cmake_guideline.md) - CMake æ„å»ºæŒ‡å—

## 13. å¸¸è§é—®é¢˜å’Œè§£ç­”

### Q1: ä¸ºä»€ä¹ˆé€‰æ‹© CPU è€Œä¸æ˜¯ GPUï¼Ÿ
**A**: CPU éƒ¨ç½²å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- æ›´å¹¿æ³›çš„éƒ¨ç½²åœºæ™¯ï¼ˆè¾¹ç¼˜è®¾å¤‡ã€æœåŠ¡å™¨ï¼‰
- æ— éœ€é¢å¤–çš„ GPU ç¡¬ä»¶æˆæœ¬
- æ›´å®¹æ˜“çš„å¼€å‘å’Œè°ƒè¯•
- å¯¹äºå°æ‰¹é‡æ¨ç†ï¼ŒCPU å¯èƒ½æ›´ç»æµ

### Q2: FastGELU çš„ç²¾åº¦æŸå¤±æœ‰å¤šå¤§ï¼Ÿ
**A**: ä½¿ç”¨ tanh è¿‘ä¼¼çš„ FastGELU ç›¸æ¯”æ ‡å‡† GELUï¼š
- æœ€å¤§ç»å¯¹è¯¯å·®ï¼š< 1e-3
- å¹³å‡ç›¸å¯¹è¯¯å·®ï¼š< 1e-4
- å¯¹äºå¤§å¤šæ•° NLP ä»»åŠ¡ï¼Œç²¾åº¦æŸå¤±å¯å¿½ç•¥
- é€Ÿåº¦æå‡ï¼š2-3x

### Q3: å¦‚ä½•é€‰æ‹©æœ€ä¼˜çš„çº¿ç¨‹æ•°ï¼Ÿ
**A**: å»ºè®®ç­–ç•¥ï¼š
- é»˜è®¤ï¼šä½¿ç”¨ç‰©ç†æ ¸å¿ƒæ•°ï¼ˆä¸åŒ…æ‹¬è¶…çº¿ç¨‹ï¼‰
- å°æ¨¡å‹ï¼ˆ< 1M å‚æ•°ï¼‰ï¼š1-2 çº¿ç¨‹
- ä¸­ç­‰æ¨¡å‹ï¼ˆ1M-100M å‚æ•°ï¼‰ï¼šç‰©ç†æ ¸å¿ƒæ•°
- å¤§æ¨¡å‹ï¼ˆ> 100M å‚æ•°ï¼‰ï¼šç‰©ç†æ ¸å¿ƒæ•°æˆ–ç¨å¤š
- é€šè¿‡å®éªŒæµ‹è¯•ä¸åŒé…ç½®

### Q4: Tiny-GPT2 ä¸æ ‡å‡† GPT-2 çš„ä¼˜åŒ–å·®å¼‚ï¼Ÿ
**A**: é’ˆå¯¹ Tiny-GPT2 çš„ç‰¹å®šä¼˜åŒ–ï¼š
- **æ›´å°çš„æ¨¡å‹** - 6 å±‚ vs 12 å±‚ï¼Œå†…å­˜å ç”¨å‡åŠ
- **æ›´é€‚åˆ CPU** - å‚æ•°é‡è¾ƒå°ï¼ŒCPU ç¼“å­˜åˆ©ç”¨ç‡æ›´é«˜
- **å•æ‰¹æ¬¡ä¼˜å…ˆ** - é’ˆå¯¹ batch=1 ä¼˜åŒ–ï¼Œé™ä½é¦– token å»¶è¿Ÿ
- **æ›´æ¿€è¿›çš„èåˆ** - ç”±äºæ¨¡å‹å°ï¼Œå¯ä»¥æ›´å¤šä½¿ç”¨ç®—å­èåˆ
- **å®æ—¶æ¨ç†** - ç›®æ ‡å»¶è¿Ÿ < 30ms/tokenï¼Œé€‚åˆäº¤äº’å¼åº”ç”¨

### Q5: å¦‚ä½•å¤„ç†ä¸åŒ CPU æ¶æ„ï¼Ÿ
**A**: ç¼–è¯‘æ—¶ç­–ç•¥ï¼š
```bash
# é€šç”¨ç‰ˆæœ¬ï¼ˆå…¼å®¹æ€§ä¼˜å…ˆï¼‰
-march=x86-64

# é’ˆå¯¹ç‰¹å®šæ¶æ„ä¼˜åŒ–ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰
-march=native  # ç¼–è¯‘æœºå™¨çš„æ¶æ„
-march=skylake # Intel Skylake
-march=znver2  # AMD Zen 2
```

### Q5: å†…å­˜å ç”¨å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
**A**: ä¼˜åŒ–ç­–ç•¥ï¼š
- ä½¿ç”¨ float16 (åŠç²¾åº¦) ä»£æ›¿ float32
- å¯ç”¨å†…å­˜å¤ç”¨ (`enable_mem_pattern=True`)
- ä½¿ç”¨æµå¼å¤„ç†é•¿åºåˆ—
- é‡åŒ–æ¨¡å‹ï¼ˆINT8ï¼‰

### Q6: å¦‚ä½•éªŒè¯ä¼˜åŒ–æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ
**A**: éªŒè¯æ–¹æ³•ï¼š
1. æ£€æŸ¥æ¨¡å‹èŠ‚ç‚¹æ•°æ˜¯å¦å‡å°‘
2. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¯¹æ¯”
3. ä½¿ç”¨ `onnxruntime_perf_test` å·¥å…·
4. æŸ¥çœ‹æ—¥å¿—ç¡®è®¤ç®—å­è¢«è°ƒç”¨
5. ä½¿ç”¨ profiler åˆ†æçƒ­ç‚¹å‡½æ•°

## 14. å®ç°è¿›åº¦æ€»ç»“

### 14.1 å½“å‰å®ç°çŠ¶æ€ï¼ˆ2025-11-18ï¼‰

#### âœ… å·²å®Œæˆçš„å·¥ä½œ

**1. ç›®å½•ç»“æ„å’Œæ–‡ä»¶** (100% å®Œæˆ)
- âœ… `my_cpu/bert/` ç›®å½•
- âœ… `test/my_cpu/` ç›®å½•
- âœ… æ‰€æœ‰å¿…éœ€çš„ .h/.cc æ–‡ä»¶
- âœ… CMakeLists.txt æ„å»ºé…ç½®
- âœ… æ–‡æ¡£å’Œå·¥å…·è„šæœ¬

**2. FastGELU ç®—å­** (100% å®Œæˆ)
- âœ… å¤´æ–‡ä»¶å®ç° (`fast_gelu.h`)
- âœ… æºæ–‡ä»¶å®ç° (`fast_gelu.cc`)
- âœ… æ ‡é‡ç‰ˆæœ¬å®ç°ï¼ˆä½¿ç”¨ std::tanhï¼‰
- âœ… Bias è¾“å…¥æ”¯æŒï¼ˆä¸ºèåˆé¢„ç•™ï¼‰
- âœ… TODO-OPTIMIZE æ ‡æ³¨ï¼ˆAVX2, OpenMPï¼‰
- âœ… æ¨¡æ¿å®ä¾‹åŒ–ï¼ˆfloatï¼‰

**3. ç®—å­æ³¨å†Œç³»ç»Ÿ** (100% å®Œæˆ)
- âœ… `my_cpu_kernels.h` - æ³¨å†Œå¤´æ–‡ä»¶
- âœ… `my_cpu_kernels.cc` - æ³¨å†Œå®ç°
- âœ… FastGelu å·²æ³¨å†Œåˆ° kMSDomain

**4. å•å…ƒæµ‹è¯•** (100% å®Œæˆ)
- âœ… `fast_gelu_op_test.cc` - å®Œæ•´æµ‹è¯•å¥—ä»¶
- âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•
- âœ… ä¸åŒå½¢çŠ¶æµ‹è¯•
- âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•
- âœ… å¤§å¼ é‡æµ‹è¯•ï¼ˆTiny-GPT2 è§„æ¨¡ï¼‰
- âœ… æ€§èƒ½æµ‹è¯•å ä½ï¼ˆTODO-OPTIMIZEï¼‰

**5. æ„å»ºç³»ç»Ÿ** (100% å®Œæˆ)
- âœ… `my_cpu/CMakeLists.txt` - åº“æ„å»º
- âœ… `test/my_cpu/CMakeLists.txt` - æµ‹è¯•æ„å»º
- âœ… ç¼–è¯‘é€‰é¡¹é…ç½®ï¼ˆAVX2 å·²é¢„ç•™ï¼‰
- âœ… ä¾èµ–é“¾æ¥é…ç½®

**6. æ–‡æ¡£å’Œå·¥å…·** (100% å®Œæˆ)
- âœ… `README.md` - ä½¿ç”¨æ–‡æ¡£ (~220 è¡Œ)
- âœ… `INTEGRATION.md` - é›†æˆæŒ‡å— (~350 è¡Œ)
- âœ… `QUICKSTART.md` - å¿«é€Ÿå‚è€ƒ (~200 è¡Œ)
- âœ… `generate_test_data.py` - æµ‹è¯•æ•°æ®ç”Ÿæˆ
- âœ… `verify.sh` / `verify.bat` - éªŒè¯è„šæœ¬

**7. ä»£ç è´¨é‡**
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†
- âœ… æ¸…æ™°çš„ä»£ç æ³¨é‡Š
- âœ… ä¸€è‡´çš„ä»£ç é£æ ¼
- âœ… TODO-OPTIMIZE æ ‡æ³¨è§„èŒƒ

#### â­ï¸ å¾…å®Œæˆçš„å·¥ä½œ

**1. é›†æˆå’Œç¼–è¯‘** (0% å®Œæˆ)
- [ ] ä¿®æ”¹ä¸» CMakeLists.txt é›†æˆ my_cpu
- [ ] ç¼–è¯‘ ONNX Runtime with my_cpu
- [ ] è¿è¡Œå•å…ƒæµ‹è¯•éªŒè¯
- [ ] ä¿®å¤å¯èƒ½çš„ç¼–è¯‘é”™è¯¯

**2. å…¶ä»–ç®—å­éªŒè¯** (0% å®Œæˆ)
- [ ] æ£€æŸ¥ LayerNormalization æ˜¯å¦å¯ç”¨
- [ ] æ£€æŸ¥ Attention æ˜¯å¦å¯ç”¨
- [ ] å†³å®šæ˜¯å¦éœ€è¦è‡ªè¡Œå®ç°

**3. ç«¯åˆ°ç«¯æµ‹è¯•** (0% å®Œæˆ)
- [ ] å¯¼å‡º Tiny-GPT2 ONNX æ¨¡å‹
- [ ] ä¼˜åŒ–æ¨¡å‹ï¼ˆç®—å­èåˆï¼‰
- [ ] è¿è¡Œæ¨ç†æµ‹è¯•
- [ ] ç²¾åº¦éªŒè¯ï¼ˆ< 1e-3 è¯¯å·®ï¼‰

**4. æ€§èƒ½ä¼˜åŒ–** (0% å®Œæˆ - å¯é€‰)
- [ ] å®ç° AVX2 SIMD ç‰ˆæœ¬
- [ ] å®ç° OpenMP å¹¶è¡ŒåŒ–
- [ ] å®ç° SkipLayerNormalization
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### 14.2 ä»£ç ç»Ÿè®¡

| ç±»åˆ« | æ–‡ä»¶æ•° | è¡Œæ•° | çŠ¶æ€ |
|------|--------|------|------|
| **æ ¸å¿ƒå®ç°** | 4 | ~400 | âœ… å®Œæˆ |
| - FastGELU å¤´æ–‡ä»¶ | 1 | ~42 | âœ… |
| - FastGELU å®ç° | 1 | ~150 | âœ… |
| - ç®—å­æ³¨å†Œå¤´æ–‡ä»¶ | 1 | ~20 | âœ… |
| - ç®—å­æ³¨å†Œå®ç° | 1 | ~40 | âœ… |
| **æµ‹è¯•** | 2 | ~200 | âœ… å®Œæˆ |
| - å•å…ƒæµ‹è¯• | 1 | ~180 | âœ… |
| - æµ‹è¯•æ„å»ºé…ç½® | 1 | ~20 | âœ… |
| **æ„å»ºç³»ç»Ÿ** | 2 | ~100 | âœ… å®Œæˆ |
| - åº“æ„å»ºé…ç½® | 1 | ~60 | âœ… |
| - æµ‹è¯•æ„å»ºé…ç½® | 1 | ~20 | âœ… |
| **æ–‡æ¡£** | 4 | ~900 | âœ… å®Œæˆ |
| - README | 1 | ~220 | âœ… |
| - INTEGRATION | 1 | ~350 | âœ… |
| - QUICKSTART | 1 | ~200 | âœ… |
| - æœ¬å®ç°è®¡åˆ’ | 1 | ~2300 | âœ… |
| **å·¥å…·è„šæœ¬** | 3 | ~200 | âœ… å®Œæˆ |
| **æ€»è®¡** | **15** | **~1,800** | **âœ… åŸºç¡€å®Œæˆ** |

### 14.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

#### ç«‹å³è¡ŒåŠ¨ï¼ˆæœ¬å‘¨ï¼‰
1. **éªŒè¯å®ç°**
   ```bash
   cd d:/open-source/onnxruntime
   bash my_cpu/verify.sh
   ```

2. **é›†æˆåˆ°æ„å»ºç³»ç»Ÿ**
   - å‚è€ƒ `my_cpu/INTEGRATION.md`
   - ä¿®æ”¹ä¸» CMakeLists.txt
   - ç¼–è¯‘æµ‹è¯•

3. **è¿è¡Œå•å…ƒæµ‹è¯•**
   ```bash
   cd build/Release
   ./onnxruntime_test_all --gtest_filter="*FastGelu*"
   ```

#### çŸ­æœŸç›®æ ‡ï¼ˆ1-2å‘¨ï¼‰
- [ ] å®Œæˆç¼–è¯‘å’Œæµ‹è¯•éªŒè¯
- [ ] éªŒè¯ LayerNormalization/Attention å¯ç”¨æ€§
- [ ] å‡†å¤‡ Tiny-GPT2 ONNX æ¨¡å‹
- [ ] è¿è¡Œç«¯åˆ°ç«¯æ¨ç†æµ‹è¯•

#### ä¸­æœŸç›®æ ‡ï¼ˆ1ä¸ªæœˆï¼‰
- [ ] Tiny-GPT2 æ­£ç¡®æ¨ç†ï¼ˆç²¾åº¦ < 1e-3ï¼‰
- [ ] åŸºç¡€æ€§èƒ½æµ‹è¯•
- [ ] å†³å®šæ˜¯å¦éœ€è¦æ€§èƒ½ä¼˜åŒ–

#### é•¿æœŸç›®æ ‡ï¼ˆå¯é€‰ï¼‰
- [ ] SIMD ä¼˜åŒ–ï¼ˆAVX2ï¼‰
- [ ] å¹¶è¡Œä¼˜åŒ–ï¼ˆOpenMPï¼‰
- [ ] èåˆç®—å­å®ç°
- [ ] æ€§èƒ½è¾¾åˆ°ç›®æ ‡ï¼ˆ< 30ms é¦– tokenï¼‰

### 14.4 é‡Œç¨‹ç¢‘æ£€æŸ¥æ¸…å•

**é‡Œç¨‹ç¢‘ 1: åŸºç¡€å®ç°** âœ… å·²å®Œæˆ
- [x] ä»£ç å®ç°å®Œæˆ
- [x] æµ‹è¯•ç¼–å†™å®Œæˆ
- [x] æ–‡æ¡£ç¼–å†™å®Œæˆ
- [x] æ„å»ºé…ç½®å®Œæˆ

**é‡Œç¨‹ç¢‘ 2: é›†æˆæµ‹è¯•** â­ï¸ è¿›è¡Œä¸­
- [ ] æˆåŠŸç¼–è¯‘
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] æ— ç¼–è¯‘/é“¾æ¥é”™è¯¯

**é‡Œç¨‹ç¢‘ 3: åŠŸèƒ½éªŒè¯** â­ï¸ å¾…å¼€å§‹
- [ ] Tiny-GPT2 æ¨¡å‹åŠ è½½
- [ ] æ¨ç†æˆåŠŸè¿è¡Œ
- [ ] ç²¾åº¦éªŒè¯é€šè¿‡

**é‡Œç¨‹ç¢‘ 4: ä¼˜åŒ–æå‡** â­ï¸ å¯é€‰
- [ ] SIMD ä¼˜åŒ–å®ç°
- [ ] æ€§èƒ½ç›®æ ‡è¾¾æˆ
- [ ] ç”Ÿäº§å°±ç»ª

---

## 15. ä¸‹ä¸€æ­¥è®¡åˆ’

### 15.1 çŸ­æœŸç›®æ ‡ï¼ˆ1-2ä¸ªæœˆï¼‰
- [ ] å®Œæˆæ‰€æœ‰æ ¸å¿ƒç®—å­çš„ CPU å®ç°
- [ ] é€šè¿‡ Tiny-GPT2 ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] è¾¾åˆ°æ€§èƒ½ç›®æ ‡ï¼š
  - é¦– token å»¶è¿Ÿ < 30ms
  - åç»­ token < 20ms
  - ç›¸å¯¹ PyTorch 1.5-2x åŠ é€Ÿ
- [ ] å®Œå–„æµ‹è¯•è¦†ç›–ç‡ï¼ˆ> 90%ï¼‰
- [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£å’Œç¤ºä¾‹

### 14.2 ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰
- [ ] æ”¯æŒå…¶ä»–è½»é‡çº§æ¨¡å‹ï¼ˆDistilGPT-2ï¼‰
- [ ] å®ç° INT8 é‡åŒ–æ”¯æŒï¼ˆè¿›ä¸€æ­¥åŠ é€Ÿï¼‰
- [ ] ä¼˜åŒ–åŠ¨æ€å½¢çŠ¶å¤„ç†
- [ ] æ·»åŠ  AVX-512 ä¼˜åŒ–è·¯å¾„
- [ ] å¼€å‘æ¨ç†æœåŠ¡ç¤ºä¾‹

### 14.3 é•¿æœŸç›®æ ‡ï¼ˆ6-12ä¸ªæœˆï¼‰
- [ ] æ”¯æŒæ›´å¤š GPT å˜ä½“ï¼ˆGPT-Neo-125Mï¼‰
- [ ] å®ç°æµå¼æ¨ç†ä¼˜åŒ–
- [ ] æ·»åŠ  ARM NEON ä¼˜åŒ–ï¼ˆè¾¹ç¼˜è®¾å¤‡ï¼‰
- [ ] é›†æˆåˆ°ç”Ÿäº§ç³»ç»Ÿ
- [ ] æ€§èƒ½ä¼˜åŒ–åˆ° < 15ms/token

## 15. è”ç³»å’Œæ”¯æŒ

### é¡¹ç›®ä¿¡æ¯
- **é¡¹ç›®åç§°**: ONNX Runtime Tiny-GPT2 CPU ä¼˜åŒ–ç®—å­
- **ç›®æ ‡æ¨¡å‹**: Tiny-GPT2-ONNX (6 å±‚, 768 éšè—ç»´åº¦, ~50M å‚æ•°)
- **ç›®æ ‡å¹³å°**: CPU (x86-64, AVX2+)
- **è®¸å¯è¯**: MIT License

### å¿«é€Ÿå¼€å§‹
```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/microsoft/onnxruntime.git
cd onnxruntime

# 2. ç¼–è¯‘ï¼ˆå¯ç”¨ CPU ä¼˜åŒ–ï¼‰
./build.sh --config Release --build_shared_lib --parallel --use_openmp

# 3. ä¼˜åŒ–ä½ çš„ Tiny-GPT2 æ¨¡å‹
python scripts/optimize_and_deploy_gpt2.py \
    --input tiny-gpt2.onnx \
    --output tiny_gpt2_optimized.onnx

# 4. è¿è¡ŒåŸºå‡†æµ‹è¯•
python onnxruntime/test/python/transformers/benchmark_tiny_gpt2.py \
    --model tiny_gpt2_optimized.onnx \
    --threads 4

# 5. æµ‹è¯•ç”Ÿæˆ
python onnxruntime/test/python/transformers/test_tiny_gpt2_custom_ops.py
```

### æŠ€æœ¯æ”¯æŒ
- **GitHub Issues**: [microsoft/onnxruntime/issues](https://github.com/microsoft/onnxruntime/issues)
- **è®ºå›**: [ONNX Runtime Discussions](https://github.com/microsoft/onnxruntime/discussions)
- **æ–‡æ¡£**: [ONNX Runtime å®˜æ–¹æ–‡æ¡£](https://onnxruntime.ai/)

### è´¡çŒ®æŒ‡å—
æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªï¼š
1. Fork é¡¹ç›®ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. ç¼–å†™æµ‹è¯•ç”¨ä¾‹
4. æäº¤ Pull Request
5. ç­‰å¾…ä»£ç è¯„å®¡

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0 - Tiny-GPT2 CPU ä¸“ç”¨ç‰ˆ
**æœ€åæ›´æ–°**: 2025-11-18
**ä½œè€…**: zhenzhong.han@qq.com
**ç›®æ ‡å¹³å°**: CPU (x86-64, AVX2+)
**ç›®æ ‡æ¨¡å‹**: Tiny-GPT2-ONNX (6 layers, 768 hidden, ~50M params)
**æ€§èƒ½ç›®æ ‡**: < 30ms TTFT, < 20ms/token, 1.5-2x speedup vs PyTorch
