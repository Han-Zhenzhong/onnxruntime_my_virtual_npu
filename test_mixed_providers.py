#!/usr/bin/env python3
"""
æµ‹è¯• my_cpu å’Œ CPU æä¾›è€…æ··åˆè¿è¡Œ Tiny-GPT2
"""
import onnxruntime as ort
import numpy as np
import time
from typing import List, Tuple

class MixedProviderTester:
    def __init__(self):
        self.results = {}

def test_provider_combination(self, providers: List[Tuple[str, dict]], name: str):
        """æµ‹è¯•ç‰¹å®šæä¾›è€…ç»„åˆ"""
        try:
            print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {name}")
            print(f"   æä¾›è€…: {[p[0] for p in providers]}")

            # åˆ›å»ºä¼šè¯
            session = ort.InferenceSession('tiny_gpt2.onnx', providers=providers)

            # æ£€æŸ¥å®é™…ä½¿ç”¨çš„æä¾›è€…
            actual_providers = session.get_providers()
            print(f"   å®é™…æä¾›è€…: {actual_providers}")

            # å‡†å¤‡è¾“å…¥
            input_name = session.get_inputs()[0].name
            if 'input_ids' in input_name.lower():
                test_input = np.random.randint(0, 1000, (1, 10), dtype=np.int64)
            else:
                input_shape = session.get_inputs()[0].shape
                test_input = np.random.randn(*input_shape).astype(np.float32)

            # æ‰§è¡Œæ¨ç†å¹¶è®¡æ—¶
            start_time = time.time()
            outputs = session.run(None, {input_name: test_input})
            inference_time = (time.time() - start_time) * 1000  # ms

            # è®°å½•ç»“æœ
            result = {
                'success': True,
                'inference_time': inference_time,
                'output_shape': outputs[0].shape,
                'providers': actual_providers,
                'output_stats': {
                    'min': float(outputs[0].min()),
                    'max': float(outputs[0].max()),
                    'mean': float(outputs[0].mean()),
                    'std': float(outputs[0].std())
                }
            }

            self.results[name] = result

            print(f"   âœ… æˆåŠŸ! è€—æ—¶: {inference_time:.2f} ms")
            print(f"   ğŸ“Š è¾“å‡º: {outputs[0].shape}, å‡å€¼: {outputs[0].mean():.4f}")

            return result

        except Exception as e:
            error_result = {
                'success': False,
                'error': str(e),
                'providers': [p[0] for p in providers]
            }
            self.results[name] = error_result
            print(f"   âŒ å¤±è´¥: {e}")
            return error_result

    def test_all_combinations(self):
        """æµ‹è¯•æ‰€æœ‰å¯èƒ½çš„æä¾›è€…ç»„åˆ"""

        print("ğŸš€ å¼€å§‹æ··åˆæä¾›è€…æµ‹è¯•")
        print("=" * 80)

        # é…ç½® 1: ä»… CPU æä¾›è€…ï¼ˆåŒ…å« my_cpu ç®—å­é›†æˆï¼‰
        self.test_provider_combination([
            ('CPUExecutionProvider', {})
        ], "CPUæä¾›è€…(å«my_cpuç®—å­)")

        # æ³¨æ„ï¼šå½“å‰ my_cpu ç®—å­å·²é›†æˆåˆ° CPUExecutionProvider ä¸­
        # æ‰€ä»¥ MyCpuExecutionProvider å¯èƒ½ä¸å­˜åœ¨
        # æˆ‘ä»¬ä¸»è¦æµ‹è¯• CPUExecutionProvider ä¸­çš„æ··åˆç®—å­ä½¿ç”¨

    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯•ç»“æœåˆ†æ")
        print("=" * 80)

        successful_configs = []
        for name, result in self.results.items():
            if result['success']:
                successful_configs.append((name, result))
                print(f"\nâœ… {name}:")
                print(f"   â±ï¸  æ¨ç†æ—¶é—´: {result['inference_time']:.2f} ms")
                print(f"   ğŸ”§ å®é™…æä¾›è€…: {result['providers']}")
                print(f"   ğŸ“Š è¾“å‡ºç»Ÿè®¡: å‡å€¼={result['output_stats']['mean']:.4f}, "
                      f"æ ‡å‡†å·®={result['output_stats']['std']:.4f}")
            else:
                print(f"\nâŒ {name}:")
                print(f"   é”™è¯¯: {result['error']}")

        if len(successful_configs) > 1:
            print(f"\nğŸ† æ€§èƒ½å¯¹æ¯”:")
            successful_configs.sort(key=lambda x: x[1]['inference_time'])
            fastest = successful_configs[0]
            print(f"   æœ€å¿«é…ç½®: {fastest[0]} ({fastest[1]['inference_time']:.2f} ms)")

            for name, result in successful_configs[1:]:
                slowdown = (result['inference_time'] / fastest[1]['inference_time'] - 1) * 100
                print(f"   {name}: +{slowdown:.1f}% ç›¸æ¯”æœ€å¿«é…ç½®")

        return successful_configs

def check_provider_registration():
    """æ£€æŸ¥æä¾›è€…æ³¨å†ŒçŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥æä¾›è€…æ³¨å†ŒçŠ¶æ€")
    print("-" * 40)

    available_providers = ort.get_available_providers()
    print(f"å¯ç”¨æä¾›è€…: {available_providers}")

    # æ£€æŸ¥ CPU æä¾›è€…ï¼ˆåº”è¯¥åŒ…å« my_cpu ç®—å­ï¼‰
    if 'CPUExecutionProvider' in available_providers:
        print("âœ… CPUExecutionProvider å¯ç”¨")
        print("ğŸ’¡ my_cpu ç®—å­åº”è¯¥å·²é›†æˆåˆ° CPUExecutionProvider ä¸­")
    else:
        print("âŒ CPUExecutionProvider ä¸å¯ç”¨")

    # æ£€æŸ¥æ˜¯å¦æœ‰ç‹¬ç«‹çš„ my_cpu æä¾›è€…
    if 'MyCpuExecutionProvider' in available_providers:
        print("âœ… MyCpuExecutionProvider ä½œä¸ºç‹¬ç«‹æä¾›è€…å­˜åœ¨")
    else:
        print("â„¹ï¸  MyCpuExecutionProvider æœªä½œä¸ºç‹¬ç«‹æä¾›è€…æ³¨å†Œ")
        print("   è¿™æ˜¯æ­£å¸¸çš„ï¼Œmy_cpu ç®—å­é›†æˆåˆ° CPUExecutionProvider ä¸­")

    return available_providers

def create_test_models():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹æ¥éªŒè¯æ··åˆä½¿ç”¨"""
    try:
        import onnx
        from onnx import helper, TensorProto

        print("\nğŸ› ï¸  åˆ›å»ºæ··åˆç®—å­æµ‹è¯•æ¨¡å‹")

        # åˆ›å»ºåŒ…å« my_cpu FastGelu + æ ‡å‡†ç®—å­çš„æ¨¡å‹
        input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 4])
        output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 4])

        # èŠ‚ç‚¹åºåˆ—ï¼šAdd -> FastGelu -> MatMul
        add_node = helper.make_node('Add', ['input', 'input'], ['add_out'])

        fastgelu_node = helper.make_node(
            'FastGelu',
            ['add_out'],
            ['gelu_out'],
            domain='com.my_virtual_npu'  # è‡ªå®šä¹‰åŸŸ
        )

        # åˆ›å»ºæƒé‡
        weight = helper.make_tensor('weight', TensorProto.FLOAT, [4, 4],
                                  np.eye(4, dtype=np.float32).flatten().tolist())

        matmul_node = helper.make_node('MatMul', ['gelu_out', 'weight'], ['output'])

        # æ„å»ºå›¾
        graph = helper.make_graph(
            [add_node, fastgelu_node, matmul_node],
            'mixed_ops_test',
            [input_tensor],
            [output_tensor],
            [weight]
        )

        # åˆ›å»ºæ¨¡å‹
        model = helper.make_model(graph)

        # æ·»åŠ  opset imports
        model.opset_import.add().CopyFrom(helper.make_opsetid('', 13))  # ONNX domain
        model.opset_import.add().CopyFrom(helper.make_opsetid('com.my_virtual_npu', 1))  # è‡ªå®šä¹‰åŸŸ

        onnx.save(model, 'mixed_ops_test.onnx')
        print("âœ… åˆ›å»ºæ··åˆç®—å­æµ‹è¯•æ¨¡å‹: mixed_ops_test.onnx")

        return True

    except ImportError:
        print("âš ï¸  éœ€è¦ onnx åº“æ¥åˆ›å»ºæµ‹è¯•æ¨¡å‹")
        return False
    except Exception as e:
        print(f"âŒ åˆ›å»ºæµ‹è¯•æ¨¡å‹å¤±è´¥: {e}")
        return False

def test_mixed_ops_model():
    """æµ‹è¯•æ··åˆç®—å­æ¨¡å‹"""
    try:
        print("\nğŸ§ª æµ‹è¯•æ··åˆç®—å­æ¨¡å‹")

        # åªæµ‹è¯• CPU æä¾›è€…ï¼ˆåŒ…å« my_cpu ç®—å­ï¼‰
        configurations = [
            ([('CPUExecutionProvider', {})], "CPU(å«my_cpuç®—å­)"),
        ]

        for providers, name in configurations:
            try:
                session = ort.InferenceSession('mixed_ops_test.onnx', providers=providers)

                # æµ‹è¯•è¾“å…¥
                test_input = np.array([[1.0, 0.5, -0.5, 2.0]], dtype=np.float32)
                output = session.run(None, {'input': test_input})[0]

                print(f"   âœ… {name}: è¾“å‡º = {output}")

            except Exception as e:
                print(f"   âŒ {name}: {e}")

    except FileNotFoundError:
        print("   âš ï¸  mixed_ops_test.onnx ä¸å­˜åœ¨ï¼Œè·³è¿‡æ··åˆç®—å­æµ‹è¯•")

if __name__ == "__main__":
    print("ğŸ”§ ONNXRuntime æ··åˆæ‰§è¡Œæä¾›è€…æµ‹è¯•")
    print("=" * 80)

    # 1. æ£€æŸ¥æä¾›è€…çŠ¶æ€
    available_providers = check_provider_registration()

    # 2. åˆ›å»ºæµ‹è¯•æ¨¡å‹
    create_test_models()

    # 3. æµ‹è¯•æ··åˆç®—å­æ¨¡å‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    test_mixed_ops_model()

    # 4. æµ‹è¯• Tiny-GPT2ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        tester = MixedProviderTester()
        tester.test_all_combinations()
        successful_configs = tester.analyze_results()

        print(f"\nğŸ‰ ç»“è®º:")
        if successful_configs:
            print(f"âœ… æ··åˆæä¾›è€…å¯ä»¥æˆåŠŸè¿è¡Œ Tiny-GPT2!")
            print(f"ğŸ’¡ æ¨èé…ç½®: my_cpu + CPU æ··åˆä½¿ç”¨")
            print(f"ğŸ”§ ç®—å­åˆ†é…: è‡ªå®šä¹‰ç®—å­ç”¨ my_cpuï¼Œæ ‡å‡†ç®—å­ç”¨ CPU")
        else:
            print(f"âŒ éœ€è¦è§£å†³æä¾›è€…æ³¨å†Œé—®é¢˜")

    except FileNotFoundError:
        print(f"\nâš ï¸  tiny_gpt2.onnx æœªæ‰¾åˆ°ï¼Œè·³è¿‡ Tiny-GPT2 æµ‹è¯•")
        print(f"ğŸ’¡ è¯·ç¡®ä¿ Tiny-GPT2 æ¨¡å‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•")

    print("=" * 80)
